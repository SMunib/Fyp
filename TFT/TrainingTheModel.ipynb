{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyZjKyl9f6AU",
        "outputId": "1f359f87-a934-4bf7-85f5-31c90a4c64d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pytorch-forecasting\n",
            "  Downloading pytorch_forecasting-1.2.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.6)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.10.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: numpy<=3.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (1.26.4)\n",
            "Collecting lightning<3.0.0,>=2.0.0 (from pytorch-forecasting)\n",
            "  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (1.13.1)\n",
            "Requirement already satisfied: pandas<3.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.3.0->pytorch-forecasting) (1.16.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_forecasting-1.2.0-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.9/181.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.4.0-py3-none-any.whl (810 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning, lightning, pytorch-forecasting\n",
            "Successfully installed lightning-2.4.0 lightning-utilities-0.11.9 pytorch-forecasting-1.2.0 pytorch-lightning-2.4.0 torchmetrics-1.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning pytorch-forecasting torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1EgqL7xjeTq",
        "outputId": "62b4f4df-69df-4897-f15a-43d52b1ffdc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/Favorita_Subset/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path = './Favorita_Subset/'"
      ],
      "metadata": {
        "id": "cCRmxQy49sba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHUTEIpmfvhl"
      },
      "outputs": [],
      "source": [
        "from pytorch_forecasting import TimeSeriesDataSet\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.models import TemporalFusionTransformer\n",
        "from pytorch_forecasting.metrics import RMSE\n",
        "from torch.utils.data import DataLoader\n",
        "from lightning.pytorch import Trainer\n",
        "from lightning.pytorch.callbacks import EarlyStopping\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "Ur9mPixjf1k6",
        "outputId": "0261a736-3fba-456e-bee4-37413874f9a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-e22ffdf3d5ed>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  load_train = torch.load(path + \"train_mini.pth\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Favorita_Subset/train_mini.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e22ffdf3d5ed>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"train_mini.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mload_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"val_mini.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Favorita_Subset/train_mini.pth'"
          ]
        }
      ],
      "source": [
        "load_train = torch.load(path + \"train_mini.pth\")\n",
        "load_val = torch.load(path + \"val_mini.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(load_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVaxIFfguEhd",
        "outputId": "4960f28d-3de1-4b4e-8a73-38a1093ddfe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TimeSeriesDataSet[length=16383003](\n",
            "\ttime_idx='time_idx',\n",
            "\ttarget='unit_sales',\n",
            "\tgroup_ids=['store_nbr', 'family', 'class'],\n",
            "\tweight=None,\n",
            "\tmax_encoder_length=30,\n",
            "\tmin_encoder_length=15,\n",
            "\tmin_prediction_idx=0,\n",
            "\tmin_prediction_length=7,\n",
            "\tmax_prediction_length=7,\n",
            "\tstatic_categoricals=['store_nbr', 'family', 'store_type', 'item_nbr', 'perishable', 'class', 'cluster'],\n",
            "\tstatic_reals=None,\n",
            "\ttime_varying_known_categoricals=['weekday', 'holiday_description', 'holiday_type', 'onpromotion'],\n",
            "\ttime_varying_known_reals=['dcoilwtico', 'Min Temperature (C)', 'Max Temperature (C)'],\n",
            "\ttime_varying_unknown_categoricals=None,\n",
            "\ttime_varying_unknown_reals=None,\n",
            "\tvariable_groups=None,\n",
            "\tconstant_fill_strategy=None,\n",
            "\tallow_missing_timesteps=True,\n",
            "\tlags=None,\n",
            "\tadd_relative_time_idx=True,\n",
            "\tadd_target_scales=False,\n",
            "\tadd_encoder_length=True,\n",
            "\ttarget_normalizer=EncoderNormalizer(\n",
            "\tmethod='standard',\n",
            "\tcenter=True,\n",
            "\tmax_length=None,\n",
            "\ttransformation=None,\n",
            "\tmethod_kwargs={}\n",
            "),\n",
            "\tcategorical_encoders={'__group_id__store_nbr': NaNLabelEncoder(add_nan=False, warn=True), '__group_id__family': NaNLabelEncoder(add_nan=False, warn=True), '__group_id__class': NaNLabelEncoder(add_nan=False, warn=True), 'store_nbr': NaNLabelEncoder(add_nan=False, warn=True), 'family': NaNLabelEncoder(add_nan=False, warn=True), 'store_type': NaNLabelEncoder(add_nan=False, warn=True), 'item_nbr': NaNLabelEncoder(add_nan=False, warn=True), 'perishable': NaNLabelEncoder(add_nan=False, warn=True), 'class': NaNLabelEncoder(add_nan=False, warn=True), 'cluster': NaNLabelEncoder(add_nan=False, warn=True), 'weekday': NaNLabelEncoder(add_nan=False, warn=True), 'holiday_description': NaNLabelEncoder(add_nan=False, warn=True), 'holiday_type': NaNLabelEncoder(add_nan=False, warn=True), 'onpromotion': NaNLabelEncoder(add_nan=False, warn=True)},\n",
            "\tscalers={'encoder_length': StandardScaler(), 'dcoilwtico': StandardScaler(), 'Min Temperature (C)': StandardScaler(), 'Max Temperature (C)': StandardScaler(), 'relative_time_idx': StandardScaler()},\n",
            "\trandomize_length=None,\n",
            "\tpredict_mode=False\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(load_train.categorical_encoders)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6i3GbWau6Kq",
        "outputId": "6ca728e4-126b-4cf2-c8a1-8891a6ffe11e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASBbXrmvgdoU"
      },
      "outputs": [],
      "source": [
        "train_loader = TimeSeriesDataSet.to_dataloader(load_train,batch_size=64,shuffle=True)\n",
        "val_loader = TimeSeriesDataSet.to_dataloader(load_val,batch_size=64,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-wJzDpmy_I6",
        "outputId": "64fbb302-aa4b-4c0a-edcd-dd06979a6ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:171: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
            "  super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "tft = TemporalFusionTransformer.from_dataset(load_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqaR0N-drDUp",
        "outputId": "c4c114da-e069-4a69-c382-2d189a7638a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TemporalFusionTransformer(\n",
            "  \t\"attention_head_size\":               4\n",
            "  \t\"categorical_groups\":                {}\n",
            "  \t\"causal_attention\":                  True\n",
            "  \t\"dataset_parameters\":                {'time_idx': 'time_idx', 'target': 'unit_sales', 'group_ids': ['store_nbr', 'family', 'class'], 'weight': None, 'max_encoder_length': 30, 'min_encoder_length': 15, 'min_prediction_idx': 0, 'min_prediction_length': 7, 'max_prediction_length': 7, 'static_categoricals': ['store_nbr', 'family', 'store_type', 'item_nbr', 'perishable', 'class', 'cluster'], 'static_reals': None, 'time_varying_known_categoricals': ['weekday', 'holiday_description', 'holiday_type', 'onpromotion'], 'time_varying_known_reals': ['dcoilwtico', 'Min Temperature (C)', 'Max Temperature (C)'], 'time_varying_unknown_categoricals': None, 'time_varying_unknown_reals': None, 'variable_groups': None, 'constant_fill_strategy': None, 'allow_missing_timesteps': True, 'lags': None, 'add_relative_time_idx': True, 'add_target_scales': False, 'add_encoder_length': True, 'target_normalizer': EncoderNormalizer(\n",
            "  \t\tmethod='standard',\n",
            "  \t\tcenter=True,\n",
            "  \t\tmax_length=None,\n",
            "  \t\ttransformation=None,\n",
            "  \t\tmethod_kwargs={}\n",
            "  \t), 'categorical_encoders': {'__group_id__store_nbr': NaNLabelEncoder(add_nan=False, warn=True), '__group_id__family': NaNLabelEncoder(add_nan=False, warn=True), '__group_id__class': NaNLabelEncoder(add_nan=False, warn=True), 'store_nbr': NaNLabelEncoder(add_nan=False, warn=True), 'family': NaNLabelEncoder(add_nan=False, warn=True), 'store_type': NaNLabelEncoder(add_nan=False, warn=True), 'item_nbr': NaNLabelEncoder(add_nan=False, warn=True), 'perishable': NaNLabelEncoder(add_nan=False, warn=True), 'class': NaNLabelEncoder(add_nan=False, warn=True), 'cluster': NaNLabelEncoder(add_nan=False, warn=True), 'weekday': NaNLabelEncoder(add_nan=False, warn=True), 'holiday_description': NaNLabelEncoder(add_nan=False, warn=True), 'holiday_type': NaNLabelEncoder(add_nan=False, warn=True), 'onpromotion': NaNLabelEncoder(add_nan=False, warn=True)}, 'scalers': {'encoder_length': StandardScaler(), 'dcoilwtico': StandardScaler(), 'Min Temperature (C)': StandardScaler(), 'Max Temperature (C)': StandardScaler(), 'relative_time_idx': StandardScaler()}, 'randomize_length': None, 'predict_mode': False}\n",
            "  \t\"dropout\":                           0.1\n",
            "  \t\"embedding_labels\":                  {'store_nbr': {'1': 0, '10': 1, '17': 2, '18': 3, '2': 4, '20': 5, '3': 6, '4': 7, '44': 8, '45': 9, '46': 10, '47': 11, '48': 12, '49': 13, '6': 14, '7': 15, '8': 16, '9': 17}, 'family': {'AUTOMOTIVE': 0, 'BABY CARE': 1, 'BEAUTY': 2, 'BEVERAGES': 3, 'BREAD/BAKERY': 4, 'CELEBRATION': 5, 'CLEANING': 6, 'DAIRY': 7, 'DELI': 8, 'EGGS': 9, 'FROZEN FOODS': 10, 'GROCERY I': 11, 'GROCERY II': 12, 'HARDWARE': 13, 'HOME AND KITCHEN I': 14, 'HOME AND KITCHEN II': 15, 'HOME APPLIANCES': 16, 'HOME CARE': 17, 'LADIESWEAR': 18, 'LAWN AND GARDEN': 19, 'LINGERIE': 20, 'LIQUOR,WINE,BEER': 21, 'MAGAZINES': 22, 'MEATS': 23, 'PERSONAL CARE': 24, 'PET SUPPLIES': 25, 'PLAYERS AND ELECTRONICS': 26, 'POULTRY': 27, 'PREPARED FOODS': 28, 'PRODUCE': 29, 'SCHOOL AND OFFICE SUPPLIES': 30, 'SEAFOOD': 31}, 'store_type': {'A': 0, 'B': 1, 'C': 2, 'D': 3}, 'item_nbr': {'1000866': 0, '1001305': 1, '1003679': 2, '1004545': 3, '1004550': 4, '1004551': 5, '1005456': 6, '1005458': 7, '1005461': 8, '1005463': 9, '1005464': 10, '1005465': 11, '1009512': 12, '1009539': 13, '1009946': 14, '1009997': 15, '1009998': 16, '1010752': 17, '1010755': 18, '1010757': 19, '1012473': 20, '1012787': 21, '1012788': 22, '1014864': 23, '1014865': 24, '1016067': 25, '1017349': 26, '1017791': 27, '1018516': 28, '1018617': 29, '1018878': 30, '1020237': 31, '1020940': 32, '1021281': 33, '1021676': 34, '1022053': 35, '1024975': 36, '1024976': 37, '1028589': 38, '1033395': 39, '103501': 40, '103520': 41, '1036189': 42, '1036317': 43, '1036320': 44, '103665': 45, '1036689': 46, '1037412': 47, '1037654': 48, '1037838': 49, '1037845': 50, '1037846': 51, '1037856': 52, '1037857': 53, '1038937': 54, '1038945': 55, '1038946': 56, '1038950': 57, '1038952': 58, '1038953': 59, '1038954': 60, '1038962': 61, '1039087': 62, '1040170': 63, '1040348': 64, '1044584': 65, '1044592': 66, '1045449': 67, '1046272': 68, '1047394': 69, '1047395': 70, '1047396': 71, '1047674': 72, '1047675': 73, '1047679': 74, '1047680': 75, '1047681': 76, '1047682': 77, '1047685': 78, '1047686': 79, '1047690': 80, '1047695': 81, '1047696': 82, '1047698': 83, '1047699': 84, '1047702': 85, '1047703': 86, '1047705': 87, '1047707': 88, '1047709': 89, '1047717': 90, '1047721': 91, '1047733': 92, '1047734': 93, '1047735': 94, '1047740': 95, '1047743': 96, '1047751': 97, '1047752': 98, '1047753': 99, '1047755': 100, '1047756': 101, '1047757': 102, '1047758': 103, '1047772': 104, '1047773': 105, '1047774': 106, '1047775': 107, '1047786': 108, '1047790': 109, '1049595': 110, '1050142': 111, '1050370': 112, '1052022': 113, '1052563': 114, '1053943': 115, '1053944': 116, '1053945': 117, '1054129': 118, '105574': 119, '105575': 120, '105576': 121, '105577': 122, '105693': 123, '1057033': 124, '1057034': 125, '105737': 126, '1057490': 127, '1057491': 128, '1057495': 129, '1057498': 130, '1057515': 131, '105857': 132, '1058820': 133, '1060036': 134, '1062338': 135, '1062374': 136, '1062926': 137, '1062927': 138, '1064018': 139, '1065599': 140, '1066900': 141, '1066901': 142, '106716': 143, '1070962': 144, '1071928': 145, '1071949': 146, '1072167': 147, '1072173': 148, '1074327': 149, '1075283': 150, '1075891': 151, '1076065': 152, '1076066': 153, '1076155': 154, '1076335': 155, '1078288': 156, '1079214': 157, '1079215': 158, '1079220': 159, '1079254': 160, '1079257': 161, '1079258': 162, '1079329': 163, '1079331': 164, '1080001': 165, '1080009': 166, '1080021': 167, '1080756': 168, '108079': 169, '1081359': 170, '1082042': 171, '1082391': 172, '1082578': 173, '1082907': 174, '1083152': 175, '1083154': 176, '1083196': 177, '1083517': 178, '1083518': 179, '1084365': 180, '1084417': 181, '1084436': 182, '1084437': 183, '1084881': 184, '1084882': 185, '1085148': 186, '1085150': 187, '1085246': 188, '1085486': 189, '1085686': 190, '1086172': 191, '108634': 192, '108696': 193, '108698': 194, '108701': 195, '1087269': 196, '1087270': 197, '1087467': 198, '108786': 199, '108797': 200, '1088002': 201, '108831': 202, '108833': 203, '108862': 204, '1089044': 205, '1089045': 206, '1089046': 207, '1089079': 208, '1089163': 209, '108952': 210, '1089820': 211, '1089843': 212, '1089844': 213, '1089845': 214, '1089846': 215, '1090401': 216, '1090419': 217, '1091333': 218, '1091338': 219, '1091365': 220, '1091366': 221, '1091367': 222, '1091368': 223, '1091369': 224, '1091370': 225, '1091467': 226, '1092008': 227, '1092061': 228, '1092106': 229, '1093340': 230, '1093344': 231, '1093786': 232, '1094235': 233, '1094238': 234, '1096235': 235, '1097187': 236, '1098624': 237, '1099990': 238, '1101724': 239, '1102268': 240, '1102970': 241, '1102971': 242, '1102975': 243, '1102976': 244, '1104598': 245, '1104599': 246, '1104650': 247, '1104725': 248, '1105210': 249, '1105211': 250, '1105212': 251, '1105213': 252, '1105214': 253, '1105215': 254, '1105228': 255, '1105299': 256, '1105831': 257, '1106968': 258, '1109116': 259, '1109173': 260, '1109211': 261, '1109235': 262, '1109325': 263, '1109326': 264, '1109389': 265, '1109390': 266, '1109391': 267, '1110679': 268, '1111202': 269, '111223': 270, '1112450': 271, '1112839': 272, '1112840': 273, '1113847': 274, '1113852': 275, '1113872': 276, '111397': 277, '1114566': 278, '1114567': 279, '1114749': 280, '1114754': 281, '1114761': 282, '1114903': 283, '1114914': 284, '1114924': 285, '1116113': 286, '1116370': 287, '1117661': 288, '1117662': 289, '1117663': 290, '1118683': 291, '1118691': 292, '1118775': 293, '1119216': 294, '1119217': 295, '1121513': 296, '1121514': 297, '1121515': 298, '1121615': 299, '1121616': 300, '1123722': 301, '1124164': 302, '1124165': 303, '1124973': 304, '1127860': 305, '1127862': 306, '112830': 307, '1132005': 308, '1133955': 309, '1134971': 310, '1135438': 311, '1136979': 312, '1137146': 313, '1138053': 314, '1141956': 315, '1143685': 316, '1143686': 317, '1143691': 318, '1145374': 319, '1145597': 320, '1146496': 321, '1146497': 322, '1146648': 323, '1146783': 324, '1146784': 325, '1146785': 326, '1146786': 327, '1146787': 328, '1146795': 329, '1146796': 330, '1146797': 331, '1146799': 332, '1146800': 333, '1146801': 334, '1146802': 335, '1146803': 336, '1146804': 337, '1146974': 338, '1147017': 339, '1147495': 340, '1147731': 341, '114778': 342, '114790': 343, '114799': 344, '114800': 345, '1148017': 346, '1148018': 347, '1148548': 348, '1148719': 349, '1148753': 350, '1148812': 351, '1148971': 352, '1148972': 353, '1149069': 354, '1149225': 355, '1149579': 356, '1150362': 357, '1150700': 358, '1150954': 359, '1151128': 360, '1152017': 361, '1152346': 362, '1152348': 363, '1152350': 364, '1152355': 365, '1152464': 366, '115267': 367, '1153584': 368, '1153597': 369, '1154749': 370, '115611': 371, '1156722': 372, '1156724': 373, '115693': 374, '115720': 375, '1157329': 376, '1157462': 377, '1157479': 378, '1157561': 379, '1157562': 380, '1157564': 381, '1157685': 382, '1157699': 383, '115847': 384, '115850': 385, '1158705': 386, '1158719': 387, '1158720': 388, '115891': 389, '115892': 390, '115893': 391, '115894': 392, '1159413': 393, '1159414': 394, '1159415': 395, '1159597': 396, '1159598': 397, '1159726': 398, '1159728': 399, '116017': 400, '116018': 401, '1160752': 402, '1160754': 403, '1160796': 404, '1160872': 405, '1160873': 406, '1160882': 407, '1161046': 408, '1161555': 409, '1161556': 410, '1161557': 411, '1161571': 412, '1161572': 413, '1162382': 414, '1162755': 415, '1162759': 416, '1162761': 417, '116279': 418, '1162932': 419, '1162933': 420, '1162934': 421, '1162935': 422, '116311': 423, '1163577': 424, '1163583': 425, '1164290': 426, '1164923': 427, '1165987': 428, '1165988': 429, '1165989': 430, '1166036': 431, '1166474': 432, '1166475': 433, '1166478': 434, '1167175': 435, '1167188': 436, '1167487': 437, '1167544': 438, '1167545': 439, '1167614': 440, '1167754': 441, '1168684': 442, '1168717': 443, '1168718': 444, '1168738': 445, '1168741': 446, '1171140': 447, '1171142': 448, '1172015': 449, '1173210': 450, '1173213': 451, '1173853': 452, '1174923': 453, '1175993': 454, '1176558': 455, '1176559': 456, '1176560': 457, '1176561': 458, '1176562': 459, '1177984': 460, '1178687': 461, '1178691': 462, '1178692': 463, '1178696': 464, '1178697': 465, '1179109': 466, '1179117': 467, '1179119': 468, '1179579': 469, '1179580': 470, '1179581': 471, '1182733': 472, '119023': 473, '119024': 474, '119026': 475, '119141': 476, '119187': 477, '119191': 478, '119193': 479, '119624': 480, '1209718': 481, '1209719': 482, '1209720': 483, '1209721': 484, '1210038': 485, '1212452': 486, '1213906': 487, '1214325': 488, '121964': 489, '122095': 490, '1221045': 491, '1221054': 492, '1222571': 493, '1222575': 494, '122418': 495, '122419': 496, '122425': 497, '1224495': 498, '1224521': 499, '1225768': 500, '1225769': 501, '1225770': 502, '1225771': 503, '1226506': 504, '1226507': 505, '1226920': 506, '1228300': 507, '1228312': 508, '1228319': 509, '1228320': 510, '1228321': 511, '1228937': 512, '1228945': 513, '1228950': 514, '1228994': 515, '1228995': 516, '1229023': 517, '1229025': 518, '1229028': 519, '1229440': 520, '1229469': 521, '1229500': 522, '1229566': 523, '1229576': 524, '1229584': 525, '1229591': 526, '1229625': 527, '1229636': 528, '1229642': 529, '1229643': 530, '1229664': 531, '1229676': 532, '1229686': 533, '1229882': 534, '1229919': 535, '1229920': 536, '1229935': 537, '1230003': 538, '1230014': 539, '1230060': 540, '1230228': 541, '1230244': 542, '1230327': 543, '1230416': 544, '1230417': 545, '1230418': 546, '1230426': 547, '1230430': 548, '1230440': 549, '1230441': 550, '1230461': 551, '1231869': 552, '1233161': 553, '123347': 554, '1234340': 555, '1235251': 556, '1235334': 557, '1235364': 558, '1235365': 559, '123601': 560, '123602': 561, '1237005': 562, '1237410': 563, '1238497': 564, '123927': 565, '1239294': 566, '1239720': 567, '1239736': 568, '1239739': 569, '1239740': 570, '1239741': 571, '1239746': 572, '1239748': 573, '1239749': 574, '1239782': 575, '1239783': 576, '1239785': 577, '1239788': 578, '1239789': 579, '1239790': 580, '1239791': 581, '1239792': 582, '1239793': 583, '1239794': 584, '1239795': 585, '1239796': 586, '1239797': 587, '1239798': 588, '1239800': 589, '1239806': 590, '1239807': 591, '1239808': 592, '1239809': 593, '1239810': 594, '1239812': 595, '1239813': 596, '1239815': 597, '1239817': 598, '1239818': 599, '1239819': 600, '1239820': 601, '1239840': 602, '1239841': 603, '1239842': 604, '1239843': 605, '1239844': 606, '1239845': 607, '1239846': 608, '1239847': 609, '1239852': 610, '1239853': 611, '1239854': 612, '1239855': 613, '1239856': 614, '1239857': 615, '1239858': 616, '1239859': 617, '1239860': 618, '1239861': 619, '1239862': 620, '1239867': 621, '1239868': 622, '1239869': 623, '1239870': 624, '1239871': 625, '1239875': 626, '1239876': 627, '1239877': 628, '1239879': 629, '1239880': 630, '1239881': 631, '1239882': 632, '1239883': 633, '1239887': 634, '1239890': 635, '1239891': 636, '1239897': 637, '1239898': 638, '1239899': 639, '1239901': 640, '1239902': 641, '1239903': 642, '1239905': 643, '1239906': 644, '1239908': 645, '1239911': 646, '1239912': 647, '1239913': 648, '1239914': 649, '1239915': 650, '1239916': 651, '1239918': 652, '1239953': 653, '1239954': 654, '1239955': 655, '1239956': 656, '1240160': 657, '1242666': 658, '1242667': 659, '1242670': 660, '1243803': 661, '1243814': 662, '1243815': 663, '1243816': 664, '1243817': 665, '1245918': 666, '1246493': 667, '1246495': 668, '1246498': 669, '1246757': 670, '1246758': 671, '1246784': 672, '1246785': 673, '1247036': 674, '1248391': 675, '1248394': 676, '1248464': 677, '1250226': 678, '1253470': 679, '1253708': 680, '1253765': 681, '1254013': 682, '125430': 683, '1254310': 684, '1255777': 685, '1257555': 686, '1259006': 687, '1259007': 688, '1259952': 689, '1260238': 690, '1260241': 691, '1260242': 692, '1261025': 693, '1261376': 694, '1273024': 695, '127534': 696, '127547': 697, '1279049': 698, '1280677': 699, '1281011': 700, '1281253': 701, '1281780': 702, '1282047': 703, '1282207': 704, '1284221': 705, '1285150': 706, '1285154': 707, '1288226': 708, '1288843': 709, '1289417': 710, '1289418': 711, '129296': 712, '129297': 713, '1294665': 714, '129635': 715, '129758': 716, '129759': 717, '1298766': 718, '1299616': 719, '1300325': 720, '1302670': 721, '1303141': 722, '1304243': 723, '1304488': 724, '1304490': 725, '1304491': 726, '1305845': 727, '1306155': 728, '1306196': 729, '1306197': 730, '1306198': 731, '1306199': 732, '1309245': 733, '1309246': 734, '1309297': 735, '1309298': 736, '1309300': 737, '1309560': 738, '1309672': 739, '1309856': 740, '1310076': 741, '1310081': 742, '1311104': 743, '1312957': 744, '1313222': 745, '1313223': 746, '1314409': 747, '1314549': 748, '1316369': 749, '1316461': 750, '1317092': 751, '1317094': 752, '1317095': 753, '1317096': 754, '1320180': 755, '1320701': 756, '1320708': 757, '1320857': 758, '1321332': 759, '1321336': 760, '1321497': 761, '1322479': 762, '1323011': 763, '1323422': 764, '1323985': 765, '1324174': 766, '1324182': 767, '1324591': 768, '1324667': 769, '1324668': 770, '1324670': 771, '1324845': 772, '1324851': 773, '1325324': 774, '1325325': 775, '1325330': 776, '1325362': 777, '1328340': 778, '1328471': 779, '1328495': 780, '1328672': 781, '1328907': 782, '1331068': 783, '1331407': 784, '1332103': 785, '1332664': 786, '1332688': 787, '1332689': 788, '1333224': 789, '1333225': 790, '1333307': 791, '1333994': 792, '1334068': 793, '1334074': 794, '1335305': 795, '1336459': 796, '1336461': 797, '1339795': 798, '1339879': 799, '1339880': 800, '1339882': 801, '1339883': 802, '1342003': 803, '1342006': 804, '1342007': 805, '1342008': 806, '1342009': 807, '1344514': 808, '1344755': 809, '1345067': 810, '1345068': 811, '1345070': 812, '1345071': 813, '1345072': 814, '1345337': 815, '1345352': 816, '1345981': 817, '1346410': 818, '1346621': 819, '1346624': 820, '1346625': 821, '1346626': 822, '1346627': 823, '1346628': 824, '1346629': 825, '1346630': 826, '1346631': 827, '1346632': 828, '1346636': 829, '1346637': 830, '1346638': 831, '1346643': 832, '1347810': 833, '1347811': 834, '1347977': 835, '1348487': 836, '1348983': 837, '1349068': 838, '1349806': 839, '1349807': 840, '1349808': 841, '1350565': 842, '1351227': 843, '1351305': 844, '1352751': 845, '1352754': 846, '1353189': 847, '1353191': 848, '1353969': 849, '1354380': 850, '1354381': 851, '1354382': 852, '1354383': 853, '1354384': 854, '1354385': 855, '1354386': 856, '1354387': 857, '1354388': 858, '1354389': 859, '1354390': 860, '1354711': 861, '1354712': 862, '1354713': 863, '1354714': 864, '1354719': 865, '1356038': 866, '1359503': 867, '1360009': 868, '1360010': 869, '1360012': 870, '1360013': 871, '1361205': 872, '1363876': 873, '1363877': 874, '1363878': 875, '1364636': 876, '1366212': 877, '1366213': 878, '1367273': 879, '1367438': 880, '1368456': 881, '1368479': 882, '1368480': 883, '1368488': 884, '1369793': 885, '1369873': 886, '1369875': 887, '1369876': 888, '1369972': 889, '1369973': 890, '1370025': 891, '1370542': 892, '1370547': 893, '1370548': 894, '1370552': 895, '1370564': 896, '1370565': 897, '1370566': 898, '1370568': 899, '1370572': 900, '1370574': 901, '1370587': 902, '1370588': 903, '1370589': 904, '1371395': 905, '1371618': 906, '1371620': 907, '1372308': 908, '1372309': 909, '1372326': 910, '1372511': 911, '1372563': 912, '1372566': 913, '1372585': 914, '1372862': 915, '1373031': 916, '1373037': 917, '1373081': 918, '1373394': 919, '1373408': 920, '1373409': 921, '1373444': 922, '1373944': 923, '1373976': 924, '1379151': 925, '1379154': 926, '1379158': 927, '1379194': 928, '1379210': 929, '1386120': 930, '1386220': 931, '1386250': 932, '1386506': 933, '1386695': 934, '1386696': 935, '1386894': 936, '1387609': 937, '1388521': 938, '1388559': 939, '1388568': 940, '1388574': 941, '1390351': 942, '1390352': 943, '1390405': 944, '1391408': 945, '1391548': 946, '1391549': 947, '1392256': 948, '1392260': 949, '1392261': 950, '1392262': 951, '1392627': 952, '1392628': 953, '1393033': 954, '1393047': 955, '1394348': 956, '1394349': 957, '1395693': 958, '1396282': 959, '1397060': 960, '1397061': 961, '1397062': 962, '1397067': 963, '1397068': 964, '1397779': 965, '1397797': 966, '1397802': 967, '1398687': 968, '1398688': 969, '1399750': 970, '1400328': 971, '1400329': 972, '1400330': 973, '1400331': 974, '1400332': 975, '1400334': 976, '1400826': 977, '1400846': 978, '1401722': 979, '1401733': 980, '1401735': 981, '1402017': 982, '1402024': 983, '1402032': 984, '1402040': 985, '1402041': 986, '1402045': 987, '1402545': 988, '1402643': 989, '1403464': 990, '1404385': 991, '1404410': 992, '1404411': 993, '1405685': 994, '1410764': 995, '1410765': 996, '1410768': 997, '1410769': 998, '1410773': 999, '1410792': 1000, '1410850': 1001, '1411065': 1002, '1412115': 1003, '1412204': 1004, '1412379': 1005, '1412392': 1006, '1412393': 1007, '1412519': 1008, '1412575': 1009, '1415679': 1010, '1416362': 1011, '1417148': 1012, '1417154': 1013, '1417697': 1014, '1418022': 1015, '1418259': 1016, '1418484': 1017, '1418842': 1018, '1418844': 1019, '1418845': 1020, '1418846': 1021, '1418847': 1022, '1420007': 1023, '1421156': 1024, '1421428': 1025, '1421429': 1026, '1421438': 1027, '1422036': 1028, '1422222': 1029, '1422284': 1030, '1422347': 1031, '1422500': 1032, '1422815': 1033, '1422817': 1034, '1422835': 1035, '1422837': 1036, '1422849': 1037, '1423681': 1038, '1426023': 1039, '1427659': 1040, '1428328': 1041, '1428330': 1042, '1428331': 1043, '1428779': 1044, '1430040': 1045, '1430041': 1046, '1430080': 1047, '1430081': 1048, '1430082': 1049, '1430083': 1050, '1441468': 1051, '1441511': 1052, '1441513': 1053, '1441514': 1054, '1441515': 1055, '1441516': 1056, '1441517': 1057, '1441817': 1058, '1442157': 1059, '1444597': 1060, '1444598': 1061, '1444600': 1062, '1444615': 1063, '1445215': 1064, '1445281': 1065, '1446726': 1066, '1447815': 1067, '1447817': 1068, '1449487': 1069, '1449645': 1070, '1452378': 1071, '1453122': 1072, '1453123': 1073, '1453124': 1074, '1454707': 1075, '1454708': 1076, '1455272': 1077, '1455482': 1078, '1455485': 1079, '1455790': 1080, '1455886': 1081, '1456171': 1082, '1456346': 1083, '1456881': 1084, '1456886': 1085, '1456909': 1086, '1456910': 1087, '1456912': 1088, '1456916': 1089, '1456917': 1090, '1456921': 1091, '1456922': 1092, '1456931': 1093, '1456935': 1094, '1456939': 1095, '1456952': 1096, '1456993': 1097, '1456994': 1098, '1456995': 1099, '1456996': 1100, '1456997': 1101, '1456998': 1102, '1457000': 1103, '1457001': 1104, '1457002': 1105, '1457003': 1106, '1457004': 1107, '1457005': 1108, '1457012': 1109, '1457020': 1110, '1457031': 1111, '1457039': 1112, '1457160': 1113, '1457168': 1114, '1457175': 1115, '1457176': 1116, '1457177': 1117, '1457178': 1118, '1457179': 1119, '1457180': 1120, '1457181': 1121, '1457182': 1122, '1457183': 1123, '1457184': 1124, '1457185': 1125, '1457192': 1126, '1457199': 1127, '1457201': 1128, '1457202': 1129, '1457217': 1130, '1457228': 1131, '1457229': 1132, '1457230': 1133, '1457231': 1134, '1457239': 1135, '1457240': 1136, '1457245': 1137, '1457247': 1138, '1457251': 1139, '1457256': 1140, '1457311': 1141, '1457328': 1142, '1457333': 1143, '1457335': 1144, '1457337': 1145, '1457338': 1146, '1457339': 1147, '1457340': 1148, '1457341': 1149, '1457342': 1150, '1457344': 1151, '1457346': 1152, '1457359': 1153, '1457396': 1154, '1457409': 1155, '1457411': 1156, '1457416': 1157, '1457417': 1158, '1457518': 1159, '1457834': 1160, '1457835': 1161, '1457840': 1162, '1457948': 1163, '1458088': 1164, '1458329': 1165, '1458461': 1166, '1458834': 1167, '1458842': 1168, '1458844': 1169, '1458847': 1170, '1458848': 1171, '1458849': 1172, '1458850': 1173, '1458851': 1174, '1458914': 1175, '1458920': 1176, '1459058': 1177, '1459067': 1178, '1459225': 1179, '1459226': 1180, '1459514': 1181, '1459515': 1182, '1459692': 1183, '1459697': 1184, '1460018': 1185, '1460019': 1186, '1460021': 1187, '1460808': 1188, '1463437': 1189, '1463441': 1190, '1463451': 1191, '1463483': 1192, '1463484': 1193, '1463507': 1194, '1463509': 1195, '1463510': 1196, '1463555': 1197, '1463566': 1198, '1463568': 1199, '1463569': 1200, '1463587': 1201, '1463591': 1202, '1463594': 1203, '1463595': 1204, '1463596': 1205, '1463602': 1206, '1463603': 1207, '1463604': 1208, '1463609': 1209, '1463610': 1210, '1463611': 1211, '1463619': 1212, '1463620': 1213, '1463621': 1214, '1463625': 1215, '1463626': 1216, '1463632': 1217, '1463637': 1218, '1463693': 1219, '1463707': 1220, '1463765': 1221, '1463767': 1222, '1463769': 1223, '1463770': 1224, '1463771': 1225, '1463772': 1226, '1463773': 1227, '1463783': 1228, '1463784': 1229, '1463786': 1230, '1463787': 1231, '1463788': 1232, '1463789': 1233, '1463790': 1234, '1463791': 1235, '1463797': 1236, '1463798': 1237, '1463799': 1238, '1463800': 1239, '1463801': 1240, '1463805': 1241, '1463806': 1242, '1463807': 1243, '1463808': 1244, '1463810': 1245, '1463812': 1246, '1463814': 1247, '1463819': 1248, '1463820': 1249, '1463823': 1250, '1463825': 1251, '1463851': 1252, '1463852': 1253, '1463855': 1254, '1463857': 1255, '1463859': 1256, '1463860': 1257, '1463862': 1258, '1463865': 1259, '1463866': 1260, '1463867': 1261, '1463881': 1262, '1463882': 1263, '1463883': 1264, '1463885': 1265, '1463886': 1266, '1463887': 1267, '1463888': 1268, '1463896': 1269, '1463897': 1270, '1463898': 1271, '1463899': 1272, '1463900': 1273, '1463914': 1274, '1463934': 1275, '1463935': 1276, '1463936': 1277, '1463992': 1278, '1463993': 1279, '1463994': 1280, '1463995': 1281, '1463998': 1282, '1464002': 1283, '1464005': 1284, '1464008': 1285, '1464010': 1286, '1464025': 1287, '1464027': 1288, '1464028': 1289, '1464029': 1290, '1464031': 1291, '1464034': 1292, '1464035': 1293, '1464038': 1294, '1464039': 1295, '1464041': 1296, '1464042': 1297, '1464043': 1298, '1464045': 1299, '1464063': 1300, '1464065': 1301, '1464066': 1302, '1464070': 1303, '1464071': 1304, '1464072': 1305, '1464073': 1306, '1464074': 1307, '1464076': 1308, '1464077': 1309, '1464079': 1310, '1464081': 1311, '1464082': 1312, '1464086': 1313, '1464087': 1314, '1464088': 1315, '1464090': 1316, '1464091': 1317, '1464092': 1318, '1464093': 1319, '1464094': 1320, '1464095': 1321, '1464097': 1322, '1464099': 1323, '1464133': 1324, '1464134': 1325, '1464137': 1326, '1464153': 1327, '1464177': 1328, '1464186': 1329, '1464188': 1330, '1464192': 1331, '1464194': 1332, '1464196': 1333, '1464198': 1334, '1464201': 1335, '1464205': 1336, '1464207': 1337, '1464208': 1338, '1464209': 1339, '1464210': 1340, '1464217': 1341, '1464218': 1342, '1464237': 1343, '1464238': 1344, '1464239': 1345, '1464240': 1346, '1464241': 1347, '1464246': 1348, '1464256': 1349, '1464257': 1350, '1464258': 1351, '1464302': 1352, '1464309': 1353, '1464312': 1354, '1464469': 1355, '1464907': 1356, '1464910': 1357, '1464919': 1358, '1464941': 1359, '1465130': 1360, '1465644': 1361, '1466039': 1362, '1466047': 1363, '1466049': 1364, '1466077': 1365, '1466281': 1366, '1466286': 1367, '1466288': 1368, '1466819': 1369, '1466826': 1370, '1466827': 1371, '1466830': 1372, '1466832': 1373, '1466833': 1374, '1466835': 1375, '1466836': 1376, '1467082': 1377, '1471143': 1378, '1471460': 1379, '1471461': 1380, '1471462': 1381, '1471722': 1382, '1471903': 1383, '1471911': 1384, '1471912': 1385, '1471913': 1386, '1471916': 1387, '1471956': 1388, '1472433': 1389, '1472451': 1390, '1472453': 1391, '1472479': 1392, '1472484': 1393, '1472485': 1394, '1472487': 1395, '1472489': 1396, '1473393': 1397, '1473394': 1398, '1473396': 1399, '1473401': 1400, '1473402': 1401, '1473403': 1402, '1473404': 1403, '1473405': 1404, '1473406': 1405, '1473409': 1406, '1473410': 1407, '1473411': 1408, '1473412': 1409, '1473413': 1410, '1473414': 1411, '1473415': 1412, '1473425': 1413, '1473427': 1414, '1473428': 1415, '1473474': 1416, '1473475': 1417, '1473476': 1418, '1473477': 1419, '1473478': 1420, '1473479': 1421, '1473480': 1422, '1473481': 1423, '1473482': 1424, '1473483': 1425, '1473484': 1426, '1473485': 1427, '1473486': 1428, '1473487': 1429, '1473488': 1430, '1473489': 1431, '1473499': 1432, '1473500': 1433, '1473509': 1434, '1473511': 1435, '1473513': 1436, '1489660': 1437, '1489662': 1438, '1489691': 1439, '1489694': 1440, '1489741': 1441, '1489833': 1442, '1489835': 1443, '1489836': 1444, '1489837': 1445, '1489838': 1446, '1489850': 1447, '1489853': 1448, '1489868': 1449, '1489869': 1450, '1489870': 1451, '1489871': 1452, '1489872': 1453, '1489873': 1454, '1489879': 1455, '1489880': 1456, '1489881': 1457, '1489882': 1458, '1489886': 1459, '1489890': 1460, '1489895': 1461, '1489897': 1462, '1489899': 1463, '1489900': 1464, '1489908': 1465, '1490588': 1466, '1492315': 1467, '1501508': 1468, '1501520': 1469, '1501521': 1470, '1501524': 1471, '1501525': 1472, '1501536': 1473, '1501541': 1474, '1501544': 1475, '1501547': 1476, '1501559': 1477, '1501570': 1478, '1501579': 1479, '1501581': 1480, '1502383': 1481, '1502391': 1482, '1502392': 1483, '1503117': 1484, '1503823': 1485, '1503844': 1486, '1503847': 1487, '1503854': 1488, '1503855': 1489, '1503891': 1490, '1503899': 1491, '1507720': 1492, '153078': 1493, '1532032': 1494, '1532033': 1495, '1532035': 1496, '153239': 1497, '153267': 1498, '1533184': 1499, '1533338': 1500, '1533357': 1501, '1533358': 1502, '1533364': 1503, '1533365': 1504, '153395': 1505, '153398': 1506, '1550461': 1507, '155499': 1508, '155500': 1509, '155600': 1510, '155601': 1511, '155607': 1512, '155610': 1513, '155621': 1514, '155625': 1515, '1575223': 1516, '1575231': 1517, '1575235': 1518, '1575237': 1519, '1575239': 1520, '1575240': 1521, '1575245': 1522, '1575246': 1523, '1575249': 1524, '1576262': 1525, '1576263': 1526, '1576265': 1527, '1576285': 1528, '1576312': 1529, '1576313': 1530, '1576314': 1531, '1576315': 1532, '1576316': 1533, '1576317': 1534, '1576318': 1535, '1576321': 1536, '1576322': 1537, '1576323': 1538, '1576324': 1539, '1576325': 1540, '1576326': 1541, '1576327': 1542, '1576328': 1543, '1576329': 1544, '1576330': 1545, '1576332': 1546, '1576589': 1547, '1579069': 1548, '1579070': 1549, '1579081': 1550, '1579082': 1551, '157956': 1552, '1583407': 1553, '1583522': 1554, '1583523': 1555, '1583526': 1556, '1583544': 1557, '1584293': 1558, '1584295': 1559, '1584297': 1560, '1584298': 1561, '1584300': 1562, '1584301': 1563, '1584303': 1564, '1584305': 1565, '1584307': 1566, '1584309': 1567, '1584344': 1568, '1584348': 1569, '1584353': 1570, '1584358': 1571, '1584363': 1572, '1584365': 1573, '1584369': 1574, '1584374': 1575, '1584379': 1576, '1584390': 1577, '1584399': 1578, '1584574': 1579, '1584575': 1580, '1584576': 1581, '1585782': 1582, '158680': 1583, '158788': 1584, '158789': 1585, '1588319': 1586, '158842': 1587, '158875': 1588, '158956': 1589, '159156': 1590, '159242': 1591, '1609719': 1592, '1609724': 1593, '1609729': 1594, '1609730': 1595, '1609836': 1596, '161288': 1597, '162066': 1598, '1621528': 1599, '1621529': 1600, '1625605': 1601, '1639937': 1602, '164036': 1603, '164037': 1604, '164088': 1605, '1642397': 1606, '1642398': 1607, '1642399': 1608, '1642400': 1609, '1642401': 1610, '1642664': 1611, '164647': 1612, '165550': 1613, '165551': 1614, '165553': 1615, '165594': 1616, '165693': 1617, '165704': 1618, '165705': 1619, '165718': 1620, '165727': 1621, '1658994': 1622, '165988': 1623, '1660186': 1624, '1660188': 1625, '1660191': 1626, '1660193': 1627, '1660194': 1628, '1660239': 1629, '1660260': 1630, '1660261': 1631, '1660271': 1632, '1660272': 1633, '1662320': 1634, '1672580': 1635, '1672581': 1636, '1673264': 1637, '1673295': 1638, '1673297': 1639, '167437': 1640, '1686635': 1641, '1686637': 1642, '1686641': 1643, '1686644': 1644, '1686650': 1645, '1686652': 1646, '1686656': 1647, '1686663': 1648, '1686664': 1649, '1686665': 1650, '1686668': 1651, '1686685': 1652, '168927': 1653, '168930': 1654, '168931': 1655, '1689854': 1656, '168989': 1657, '1689920': 1658, '1689921': 1659, '1689929': 1660, '169028': 1661, '169101': 1662, '169104': 1663, '1693644': 1664, '1693645': 1665, '1693647': 1666, '1693648': 1667, '1693652': 1668, '1693653': 1669, '1693656': 1670, '1693657': 1671, '1695813': 1672, '1695815': 1673, '1695816': 1674, '1695820': 1675, '1695823': 1676, '1695825': 1677, '1695828': 1678, '1695830': 1679, '1695831': 1680, '1695835': 1681, '1695836': 1682, '1695837': 1683, '1695840': 1684, '1695841': 1685, '1695844': 1686, '1695845': 1687, '1695846': 1688, '1695847': 1689, '1695848': 1690, '1695860': 1691, '1695865': 1692, '1695872': 1693, '1695873': 1694, '1695874': 1695, '1695875': 1696, '1695879': 1697, '1695882': 1698, '1695886': 1699, '1695887': 1700, '1695888': 1701, '1695900': 1702, '1695901': 1703, '1695904': 1704, '1695905': 1705, '1695907': 1706, '1695912': 1707, '1695925': 1708, '1695931': 1709, '1695932': 1710, '1695935': 1711, '1695936': 1712, '1695940': 1713, '1695950': 1714, '1695956': 1715, '1695959': 1716, '1695961': 1717, '1695964': 1718, '1695965': 1719, '1695966': 1720, '1695971': 1721, '1695972': 1722, '1695978': 1723, '1695979': 1724, '1695983': 1725, '1695984': 1726, '1695989': 1727, '1695991': 1728, '1695994': 1729, '1695995': 1730, '1695999': 1731, '1696003': 1732, '1696007': 1733, '1696008': 1734, '1696013': 1735, '1696014': 1736, '1696015': 1737, '1696020': 1738, '1696021': 1739, '1696022': 1740, '1696023': 1741, '1696025': 1742, '1696026': 1743, '1696029': 1744, '1696036': 1745, '1696038': 1746, '1696039': 1747, '1696041': 1748, '1696042': 1749, '1696044': 1750, '1696046': 1751, '1696047': 1752, '1696051': 1753, '1696052': 1754, '1707160': 1755, '170791': 1756, '1709252': 1757, '1711747': 1758, '1717057': 1759, '1717134': 1760, '1717181': 1761, '1717269': 1762, '1717271': 1763, '1717640': 1764, '1717663': 1765, '1718310': 1766, '1718319': 1767, '171890': 1768, '1719236': 1769, '1719504': 1770, '172184': 1771, '172343': 1772, '1726000': 1773, '1726002': 1774, '1726009': 1775, '1726956': 1776, '172995': 1777, '1730015': 1778, '173111': 1779, '173113': 1780, '174081': 1781, '1741423': 1782, '1751174': 1783, '1751175': 1784, '1751176': 1785, '176411': 1786, '177050': 1787, '177395': 1788, '179274': 1789, '179564': 1790, '179579': 1791, '179600': 1792, '179615': 1793, '1898959': 1794, '1898960': 1795, '1898961': 1796, '1898998': 1797, '1899450': 1798, '1900715': 1799, '1903391': 1800, '1903428': 1801, '1903432': 1802, '1903500': 1803, '1903501': 1804, '1903717': 1805, '1904678': 1806, '1905638': 1807, '1905639': 1808, '1905800': 1809, '1906099': 1810, '1906100': 1811, '1906821': 1812, '1908028': 1813, '1908340': 1814, '1909409': 1815, '1909411': 1816, '1909412': 1817, '1909453': 1818, '1909482': 1819, '1909483': 1820, '1909699': 1821, '1909708': 1822, '1909724': 1823, '1909729': 1824, '1909748': 1825, '1909752': 1826, '1909754': 1827, '1909755': 1828, '1909770': 1829, '1909901': 1830, '1910055': 1831, '1910611': 1832, '1911102': 1833, '1911336': 1834, '1912671': 1835, '1913055': 1836, '1913230': 1837, '1913236': 1838, '1913257': 1839, '1913258': 1840, '1913462': 1841, '1913463': 1842, '1913464': 1843, '1913595': 1844, '1913596': 1845, '1913599': 1846, '1913600': 1847, '1913604': 1848, '1913813': 1849, '1914276': 1850, '1914277': 1851, '1914278': 1852, '1914279': 1853, '1914280': 1854, '1914367': 1855, '1914503': 1856, '1914782': 1857, '1914786': 1858, '1915000': 1859, '1915036': 1860, '1915102': 1861, '1916165': 1862, '1916208': 1863, '1916209': 1864, '1916577': 1865, '1916578': 1866, '1916579': 1867, '1917879': 1868, '1918621': 1869, '1919795': 1870, '1919977': 1871, '1920016': 1872, '1920035': 1873, '1920045': 1874, '1920046': 1875, '1920057': 1876, '1920059': 1877, '1920067': 1878, '1920069': 1879, '1920071': 1880, '1920072': 1881, '1920080': 1882, '1920084': 1883, '1920095': 1884, '1920096': 1885, '1920107': 1886, '1920117': 1887, '1920141': 1888, '1920145': 1889, '1920284': 1890, '1921769': 1891, '1921770': 1892, '1922657': 1893, '1923218': 1894, '1924591': 1895, '1926217': 1896, '1929795': 1897, '1929797': 1898, '1930065': 1899, '1930967': 1900, '1931079': 1901, '1932491': 1902, '1932492': 1903, '1932980': 1904, '1933240': 1905, '1935459': 1906, '1935465': 1907, '1936173': 1908, '1936589': 1909, '1936629': 1910, '1936647': 1911, '1936771': 1912, '1936931': 1913, '1937039': 1914, '1937083': 1915, '1937350': 1916, '1937443': 1917, '1937454': 1918, '1937611': 1919, '1938600': 1920, '1939144': 1921, '1939221': 1922, '1939239': 1923, '1939383': 1924, '1939424': 1925, '1939531': 1926, '1940454': 1927, '1940466': 1928, '1946668': 1929, '1948305': 1930, '1950301': 1931, '1954663': 1932, '1956622': 1933, '1958166': 1934, '1958171': 1935, '1958175': 1936, '1958177': 1937, '1958179': 1938, '1958181': 1939, '1958185': 1940, '1958186': 1941, '1958187': 1942, '1958189': 1943, '1958191': 1944, '1958193': 1945, '1958200': 1946, '1958201': 1947, '1958202': 1948, '1958210': 1949, '1958216': 1950, '1958217': 1951, '1958218': 1952, '1958219': 1953, '1958223': 1954, '1958226': 1955, '1958231': 1956, '1958238': 1957, '1958245': 1958, '1958249': 1959, '1958252': 1960, '1960304': 1961, '1960591': 1962, '1960613': 1963, '1960892': 1964, '1961065': 1965, '1961112': 1966, '1963330': 1967, '1963363': 1968, '1963404': 1969, '1963459': 1970, '1963684': 1971, '1963752': 1972, '1964111': 1973, '1964850': 1974, '1965358': 1975, '1967642': 1976, '1967660': 1977, '1967914': 1978, '1968167': 1979, '1968318': 1980, '1968368': 1981, '1968390': 1982, '1968452': 1983, '1969477': 1984, '1969485': 1985, '1969863': 1986, '1969899': 1987, '1970094': 1988, '1971730': 1989, '1971792': 1990, '1972138': 1991, '1972227': 1992, '1972443': 1993, '1972462': 1994, '1972587': 1995, '1972601': 1996, '1972673': 1997, '1972737': 1998, '1972763': 1999, '1972782': 2000, '1972822': 2001, '1975439': 2002, '1975468': 2003, '1975512': 2004, '1975514': 2005, '1975527': 2006, '1975558': 2007, '1975562': 2008, '1975571': 2009, '1975578': 2010, '1975584': 2011, '1975598': 2012, '1975649': 2013, '1975680': 2014, '1975686': 2015, '1975715': 2016, '1988377': 2017, '1988975': 2018, '1990768': 2019, '1990881': 2020, '1991151': 2021, '1991217': 2022, '2000873': 2023, '2002043': 2024, '2002136': 2025, '2002166': 2026, '2007088': 2027, '2007117': 2028, '2007133': 2029, '2007186': 2030, '2007250': 2031, '2007265': 2032, '2010125': 2033, '2010127': 2034, '2010192': 2035, '2010216': 2036, '2010233': 2037, '2010235': 2038, '2010260': 2039, '2010269': 2040, '2010278': 2041, '2010282': 2042, '2010290': 2043, '2010329': 2044, '2010356': 2045, '2010359': 2046, '2010370': 2047, '2010379': 2048, '2010404': 2049, '2010419': 2050, '2010428': 2051, '2010456': 2052, '2010487': 2053, '2010498': 2054, '2010511': 2055, '2010519': 2056, '2010568': 2057, '2010574': 2058, '2010598': 2059, '2010615': 2060, '2010652': 2061, '2010667': 2062, '2010676': 2063, '2010693': 2064, '2010743': 2065, '2010755': 2066, '2010756': 2067, '2010762': 2068, '2010766': 2069, '2010779': 2070, '2010786': 2071, '2010837': 2072, '2010880': 2073, '2010885': 2074, '2010902': 2075, '2010913': 2076, '2010916': 2077, '2010918': 2078, '2010945': 2079, '2010959': 2080, '2010964': 2081, '2010972': 2082, '2010989': 2083, '2011008': 2084, '2011032': 2085, '2011054': 2086, '2011073': 2087, '2011080': 2088, '2011105': 2089, '2011110': 2090, '2011114': 2091, '2011127': 2092, '2011148': 2093, '2011153': 2094, '2011155': 2095, '2011200': 2096, '2011208': 2097, '2011211': 2098, '2011218': 2099, '2011220': 2100, '2011225': 2101, '2011243': 2102, '2011259': 2103, '2011266': 2104, '2011328': 2105, '2011329': 2106, '2011399': 2107, '2011401': 2108, '2011449': 2109, '2011463': 2110, '2012970': 2111, '2016716': 2112, '2016737': 2113, '2016738': 2114, '2018245': 2115, '2026432': 2116, '2026444': 2117, '2026454': 2118, '2026501': 2119, '2026597': 2120, '2026631': 2121, '2026650': 2122, '2026683': 2123, '2026801': 2124, '2026858': 2125, '2026893': 2126, '2026945': 2127, '2026959': 2128, '2026983': 2129, '2027090': 2130, '2027252': 2131, '2029457': 2132, '2029543': 2133, '2030002': 2134, '2036676': 2135, '2037487': 2136, '205209': 2137, '205381': 2138, '205387': 2139, '205486': 2140, '205557': 2141, '207857': 2142, '208384': 2143, '208386': 2144, '208426': 2145, '208498': 2146, '208514': 2147, '208530': 2148, '208539': 2149, '208659': 2150, '208699': 2151, '209085': 2152, '210520': 2153, '210798': 2154, '211203': 2155, '211205': 2156, '211206': 2157, '211999': 2158, '212550': 2159, '212552': 2160, '213066': 2161, '213652': 2162, '213653': 2163, '213788': 2164, '214381': 2165, '214859': 2166, '214860': 2167, '214862': 2168, '215303': 2169, '215304': 2170, '215327': 2171, '215331': 2172, '215332': 2173, '215352': 2174, '215356': 2175, '215370': 2176, '215468': 2177, '215896': 2178, '217827': 2179, '218726': 2180, '218728': 2181, '219147': 2182, '219150': 2183, '219152': 2184, '220432': 2185, '220435': 2186, '220613': 2187, '221506': 2188, '222879': 2189, '222975': 2190, '223068': 2191, '223136': 2192, '223434': 2193, '223463': 2194, '224081': 2195, '226126': 2196, '226857': 2197, '227111': 2198, '229368': 2199, '246052': 2200, '250782': 2201, '252698': 2202, '252970': 2203, '252972': 2204, '253103': 2205, '253145': 2206, '255161': 2207, '255198': 2208, '255199': 2209, '255202': 2210, '257847': 2211, '257848': 2212, '258268': 2213, '258376': 2214, '258395': 2215, '258396': 2216, '258411': 2217, '258537': 2218, '260628': 2219, '260669': 2220, '260893': 2221, '261052': 2222, '261053': 2223, '261700': 2224, '262358': 2225, '262991': 2226, '263583': 2227, '264299': 2228, '264300': 2229, '264305': 2230, '264452': 2231, '264576': 2232, '264751': 2233, '264752': 2234, '264753': 2235, '265086': 2236, '265237': 2237, '265254': 2238, '265257': 2239, '265258': 2240, '265266': 2241, '265279': 2242, '265382': 2243, '265559': 2244, '265672': 2245, '265700': 2246, '268440': 2247, '268443': 2248, '268446': 2249, '268623': 2250, '268662': 2251, '268664': 2252, '268676': 2253, '268834': 2254, '269029': 2255, '269036': 2256, '269084': 2257, '269286': 2258, '269287': 2259, '270522': 2260, '271478': 2261, '271479': 2262, '273528': 2263, '273859': 2264, '275823': 2265, '275826': 2266, '276560': 2267, '278806': 2268, '279125': 2269, '279137': 2270, '279157': 2271, '279494': 2272, '302824': 2273, '302952': 2274, '302995': 2275, '302997': 2276, '302998': 2277, '305074': 2278, '305079': 2279, '305080': 2280, '305081': 2281, '305227': 2282, '305229': 2283, '305234': 2284, '305237': 2285, '305341': 2286, '305344': 2287, '305345': 2288, '307740': 2289, '308085': 2290, '308726': 2291, '308766': 2292, '308916': 2293, '308924': 2294, '310644': 2295, '310647': 2296, '310671': 2297, '310790': 2298, '311954': 2299, '311994': 2300, '312113': 2301, '312317': 2302, '312324': 2303, '312596': 2304, '313641': 2305, '314384': 2306, '314393': 2307, '314570': 2308, '314879': 2309, '314880': 2310, '314881': 2311, '315176': 2312, '315178': 2313, '315179': 2314, '315180': 2315, '315220': 2316, '315221': 2317, '315277': 2318, '315279': 2319, '315320': 2320, '315322': 2321, '315460': 2322, '315463': 2323, '315473': 2324, '315474': 2325, '315712': 2326, '315713': 2327, '315753': 2328, '317147': 2329, '317285': 2330, '318787': 2331, '318847': 2332, '318932': 2333, '318935': 2334, '319093': 2335, '319094': 2336, '319095': 2337, '319272': 2338, '320681': 2339, '320682': 2340, '321798': 2341, '322094': 2342, '322095': 2343, '323013': 2344, '323217': 2345, '323227': 2346, '323921': 2347, '324206': 2348, '326951': 2349, '329070': 2350, '329071': 2351, '329361': 2352, '329362': 2353, '329364': 2354, '329365': 2355, '329397': 2356, '346065': 2357, '346066': 2358, '346406': 2359, '352513': 2360, '354918': 2361, '354919': 2362, '354922': 2363, '354964': 2364, '354971': 2365, '355090': 2366, '355091': 2367, '357961': 2368, '357962': 2369, '357963': 2370, '357996': 2371, '358096': 2372, '358100': 2373, '358134': 2374, '358231': 2375, '358232': 2376, '358264': 2377, '358272': 2378, '358515': 2379, '358519': 2380, '358561': 2381, '358564': 2382, '359913': 2383, '360313': 2384, '360314': 2385, '360703': 2386, '360704': 2387, '360705': 2388, '361502': 2389, '362035': 2390, '362548': 2391, '363049': 2392, '363868': 2393, '363889': 2394, '363895': 2395, '364413': 2396, '364606': 2397, '364724': 2398, '364738': 2399, '364829': 2400, '364832': 2401, '364834': 2402, '364865': 2403, '365138': 2404, '365139': 2405, '365140': 2406, '365264': 2407, '368136': 2408, '368139': 2409, '368140': 2410, '368213': 2411, '368260': 2412, '368270': 2413, '368419': 2414, '368424': 2415, '368624': 2416, '368628': 2417, '369432': 2418, '370042': 2419, '371130': 2420, '371218': 2421, '371434': 2422, '371437': 2423, '371438': 2424, '372369': 2425, '372480': 2426, '372924': 2427, '372937': 2428, '373152': 2429, '374464': 2430, '376194': 2431, '376427': 2432, '376483': 2433, '377978': 2434, '378330': 2435, '378332': 2436, '378685': 2437, '401817': 2438, '401911': 2439, '402136': 2440, '402174': 2441, '402175': 2442, '402299': 2443, '404332': 2444, '404333': 2445, '404334': 2446, '404336': 2447, '404439': 2448, '404442': 2449, '404454': 2450, '405304': 2451, '406965': 2452, '407499': 2453, '407500': 2454, '407533': 2455, '407542': 2456, '407552': 2457, '407608': 2458, '407769': 2459, '409738': 2460, '409739': 2461, '409904': 2462, '410150': 2463, '410257': 2464, '410866': 2465, '411487': 2466, '411555': 2467, '411557': 2468, '413557': 2469, '413986': 2470, '413987': 2471, '414305': 2472, '414353': 2473, '414354': 2474, '414421': 2475, '414425': 2476, '414426': 2477, '414454': 2478, '414455': 2479, '414478': 2480, '414620': 2481, '414621': 2482, '414623': 2483, '414750': 2484, '414752': 2485, '414888': 2486, '416252': 2487, '417763': 2488, '417765': 2489, '417835': 2490, '418024': 2491, '418025': 2492, '418026': 2493, '418235': 2494, '418238': 2495, '419729': 2496, '420720': 2497, '421066': 2498, '422452': 2499, '426155': 2500, '428054': 2501, '428394': 2502, '428488': 2503, '452022': 2504, '452078': 2505, '452079': 2506, '452212': 2507, '454326': 2508, '454413': 2509, '454414': 2510, '454416': 2511, '454426': 2512, '454589': 2513, '454593': 2514, '454596': 2515, '456870': 2516, '456875': 2517, '457424': 2518, '457425': 2519, '457471': 2520, '457556': 2521, '457574': 2522, '457688': 2523, '457727': 2524, '457928': 2525, '458029': 2526, '459762': 2527, '459801': 2528, '459804': 2529, '460804': 2530, '461432': 2531, '461752': 2532, '462673': 2533, '463458': 2534, '463598': 2535, '463816': 2536, '463901': 2537, '463903': 2538, '464112': 2539, '464263': 2540, '464302': 2541, '464333': 2542, '464334': 2543, '464336': 2544, '464339': 2545, '464356': 2546, '464374': 2547, '464385': 2548, '464536': 2549, '464906': 2550, '464907': 2551, '464940': 2552, '467808': 2553, '467887': 2554, '468057': 2555, '468058': 2556, '468265': 2557, '469643': 2558, '470624': 2559, '470625': 2560, '470760': 2561, '470762': 2562, '471980': 2563, '471985': 2564, '472314': 2565, '472526': 2566, '472645': 2567, '472658': 2568, '476261': 2569, '479200': 2570, '502067': 2571, '502182': 2572, '502224': 2573, '502227': 2574, '502230': 2575, '502331': 2576, '504457': 2577, '506717': 2578, '507354': 2579, '507457': 2580, '507478': 2581, '507479': 2582, '507641': 2583, '507867': 2584, '507870': 2585, '507927': 2586, '507958': 2587, '507969': 2588, '508069': 2589, '508947': 2590, '509872': 2591, '509877': 2592, '510052': 2593, '510054': 2594, '510055': 2595, '510057': 2596, '511172': 2597, '511394': 2598, '511397': 2599, '513379': 2600, '513853': 2601, '513855': 2602, '514144': 2603, '514149': 2604, '514172': 2605, '514242': 2606, '514327': 2607, '514444': 2608, '514445': 2609, '514446': 2610, '514597': 2611, '514729': 2612, '514730': 2613, '514757': 2614, '517615': 2615, '517903': 2616, '517904': 2617, '517905': 2618, '517906': 2619, '517909': 2620, '518091': 2621, '518094': 2622, '519560': 2623, '519718': 2624, '521818': 2625, '522383': 2626, '522721': 2627, '522941': 2628, '523054': 2629, '525887': 2630, '525902': 2631, '527757': 2632, '546043': 2633, '551893': 2634, '552050': 2635, '554047': 2636, '554048': 2637, '554049': 2638, '554143': 2639, '554144': 2640, '554145': 2641, '554152': 2642, '556698': 2643, '557241': 2644, '557256': 2645, '557257': 2646, '557285': 2647, '557286': 2648, '557289': 2649, '557305': 2650, '557408': 2651, '557420': 2652, '557810': 2653, '558730': 2654, '559044': 2655, '559493': 2656, '559494': 2657, '559680': 2658, '559682': 2659, '559870': 2660, '559872': 2661, '559873': 2662, '562568': 2663, '563654': 2664, '563966': 2665, '564110': 2666, '564272': 2667, '564274': 2668, '564287': 2669, '564288': 2670, '564533': 2671, '564534': 2672, '567579': 2673, '567588': 2674, '567623': 2675, '567644': 2676, '567750': 2677, '567778': 2678, '567779': 2679, '567781': 2680, '567787': 2681, '568133': 2682, '570869': 2683, '570870': 2684, '570917': 2685, '570922': 2686, '572838': 2687, '573832': 2688, '574898': 2689, '575630': 2690, '575675': 2691, '575863': 2692, '575875': 2693, '575876': 2694, '577740': 2695, '577741': 2696, '577743': 2697, '577745': 2698, '578097': 2699, '578467': 2700, '579759': 2701, '580929': 2702, '581078': 2703, '581742': 2704, '582863': 2705, '582864': 2706, '582865': 2707, '583795': 2708, '583797': 2709, '583799': 2710, '583868': 2711, '583925': 2712, '583927': 2713, '583931': 2714, '583968': 2715, '583973': 2716, '583975': 2717, '583977': 2718, '583978': 2719, '583985': 2720, '584026': 2721, '584027': 2722, '584028': 2723, '584030': 2724, '584032': 2725, '584076': 2726, '584077': 2727, '584078': 2728, '584099': 2729, '584101': 2730, '584103': 2731, '584123': 2732, '584125': 2733, '584126': 2734, '584129': 2735, '584188': 2736, '584246': 2737, '586423': 2738, '586823': 2739, '586824': 2740, '586911': 2741, '586967': 2742, '586969': 2743, '587069': 2744, '587104': 2745, '587156': 2746, '587157': 2747, '587186': 2748, '589403': 2749, '592801': 2750, '592942': 2751, '594045': 2752, '598414': 2753, '598492': 2754, '603255': 2755, '608035': 2756, '617092': 2757, '617096': 2758, '617103': 2759, '617113': 2760, '617548': 2761, '617763': 2762, '619965': 2763, '619969': 2764, '621300': 2765, '622958': 2766, '623316': 2767, '623317': 2768, '623330': 2769, '627263': 2770, '627885': 2771, '627887': 2772, '633735': 2773, '633977': 2774, '633996': 2775, '633997': 2776, '634008': 2777, '634009': 2778, '634010': 2779, '634014': 2780, '634015': 2781, '634054': 2782, '636846': 2783, '637016': 2784, '638240': 2785, '638296': 2786, '638308': 2787, '638977': 2788, '638978': 2789, '639586': 2790, '640286': 2791, '641042': 2792, '641627': 2793, '645294': 2794, '648313': 2795, '651326': 2796, '651523': 2797, '651525': 2798, '655427': 2799, '655744': 2800, '655749': 2801, '655840': 2802, '657869': 2803, '658608': 2804, '658609': 2805, '660310': 2806, '660311': 2807, '660312': 2808, '660318': 2809, '660319': 2810, '660320': 2811, '660321': 2812, '660323': 2813, '660502': 2814, '660503': 2815, '661582': 2816, '663084': 2817, '664537': 2818, '664539': 2819, '664543': 2820, '664545': 2821, '668752': 2822, '668753': 2823, '668754': 2824, '671039': 2825, '671051': 2826, '671055': 2827, '671062': 2828, '671066': 2829, '671067': 2830, '671068': 2831, '671076': 2832, '671079': 2833, '671083': 2834, '671084': 2835, '671684': 2836, '671699': 2837, '671706': 2838, '671708': 2839, '671908': 2840, '673306': 2841, '673415': 2842, '673496': 2843, '675524': 2844, '679442': 2845, '679602': 2846, '679603': 2847, '679604': 2848, '679926': 2849, '681606': 2850, '682321': 2851, '682884': 2852, '683720': 2853, '683721': 2854, '683722': 2855, '683723': 2856, '686034': 2857, '686035': 2858, '686036': 2859, '687549': 2860, '688279': 2861, '691945': 2862, '692104': 2863, '692105': 2864, '692106': 2865, '692531': 2866, '692537': 2867, '695758': 2868, '696183': 2869, '698642': 2870, '698643': 2871, '699686': 2872, '699687': 2873, '699688': 2874, '699689': 2875, '699692': 2876, '699693': 2877, '699703': 2878, '699745': 2879, '700607': 2880, '700609': 2881, '700610': 2882, '716241': 2883, '716242': 2884, '716245': 2885, '716250': 2886, '716958': 2887, '723184': 2888, '723210': 2889, '724003': 2890, '724359': 2891, '724360': 2892, '724403': 2893, '724498': 2894, '729337': 2895, '730258': 2896, '730259': 2897, '732006': 2898, '732007': 2899, '736128': 2900, '738219': 2901, '740713': 2902, '740714': 2903, '740722': 2904, '740723': 2905, '740726': 2906, '740728': 2907, '740729': 2908, '741201': 2909, '741922': 2910, '742735': 2911, '742738': 2912, '743494': 2913, '743496': 2914, '743497': 2915, '749421': 2916, '749720': 2917, '749729': 2918, '750855': 2919, '750856': 2920, '750857': 2921, '751759': 2922, '752167': 2923, '752987': 2924, '754103': 2925, '757880': 2926, '757888': 2927, '757889': 2928, '758861': 2929, '759651': 2930, '759657': 2931, '759694': 2932, '759697': 2933, '759698': 2934, '759890': 2935, '759893': 2936, '759894': 2937, '760319': 2938, '762405': 2939, '764438': 2940, '764439': 2941, '765092': 2942, '765517': 2943, '765518': 2944, '765519': 2945, '765520': 2946, '767183': 2947, '767184': 2948, '769312': 2949, '769314': 2950, '770447': 2951, '770449': 2952, '770453': 2953, '770456': 2954, '770461': 2955, '770463': 2956, '770511': 2957, '771156': 2958, '771157': 2959, '776057': 2960, '776622': 2961, '779122': 2962, '781794': 2963, '781796': 2964, '781797': 2965, '781798': 2966, '781936': 2967, '783243': 2968, '787426': 2969, '788172': 2970, '788708': 2971, '789223': 2972, '789224': 2973, '789904': 2974, '789905': 2975, '789906': 2976, '789907': 2977, '793341': 2978, '793345': 2979, '793346': 2980, '794245': 2981, '795610': 2982, '795611': 2983, '795612': 2984, '796394': 2985, '796395': 2986, '796396': 2987, '799461': 2988, '800958': 2989, '801217': 2990, '801934': 2991, '802831': 2992, '802832': 2993, '802833': 2994, '803101': 2995, '804098': 2996, '804503': 2997, '804932': 2998, '804974': 2999, '805310': 3000, '805311': 3001, '805321': 3002, '805788': 3003, '807493': 3004, '807495': 3005, '809183': 3006, '809184': 3007, '809882': 3008, '812716': 3009, '812725': 3010, '812726': 3011, '812728': 3012, '812735': 3013, '812751': 3014, '812757': 3015, '812769': 3016, '813167': 3017, '813760': 3018, '813769': 3019, '813770': 3020, '816606': 3021, '816679': 3022, '818588': 3023, '818590': 3024, '819194': 3025, '819195': 3026, '819206': 3027, '819209': 3028, '819227': 3029, '819228': 3030, '819230': 3031, '819231': 3032, '819932': 3033, '819934': 3034, '820003': 3035, '820006': 3036, '820500': 3037, '820645': 3038, '821186': 3039, '823245': 3040, '827911': 3041, '828204': 3042, '828630': 3043, '828632': 3044, '829389': 3045, '829414': 3046, '829658': 3047, '830624': 3048, '830625': 3049, '830749': 3050, '830797': 3051, '830806': 3052, '830829': 3053, '833395': 3054, '833396': 3055, '833644': 3056, '838215': 3057, '838216': 3058, '838217': 3059, '838400': 3060, '838406': 3061, '838407': 3062, '838408': 3063, '838409': 3064, '838412': 3065, '838413': 3066, '838415': 3067, '838832': 3068, '838842': 3069, '838875': 3070, '839362': 3071, '839363': 3072, '839364': 3073, '839367': 3074, '839544': 3075, '839545': 3076, '841197': 3077, '841514': 3078, '841570': 3079, '841583': 3080, '841607': 3081, '841612': 3082, '841614': 3083, '841667': 3084, '841841': 3085, '841842': 3086, '843411': 3087, '843459': 3088, '843462': 3089, '843585': 3090, '843589': 3091, '843607': 3092, '843608': 3093, '843609': 3094, '843610': 3095, '844017': 3096, '844019': 3097, '844793': 3098, '844795': 3099, '846163': 3100, '846565': 3101, '847569': 3102, '847575': 3103, '847859': 3104, '847863': 3105, '847894': 3106, '848765': 3107, '848953': 3108, '849080': 3109, '849095': 3110, '849098': 3111, '849135': 3112, '849136': 3113, '849139': 3114, '849142': 3115, '849312': 3116, '849941': 3117, '850333': 3118, '850334': 3119, '850388': 3120, '850389': 3121, '850460': 3122, '850542': 3123, '850545': 3124, '851982': 3125, '852110': 3126, '852620': 3127, '852934': 3128, '852937': 3129, '852938': 3130, '853195': 3131, '853196': 3132, '856687': 3133, '856688': 3134, '857324': 3135, '857327': 3136, '857331': 3137, '857332': 3138, '858872': 3139, '859529': 3140, '859932': 3141, '859966': 3142, '860548': 3143, '860549': 3144, '862213': 3145, '862454': 3146, '864288': 3147, '864290': 3148, '864508': 3149, '864510': 3150, '864511': 3151, '865147': 3152, '865155': 3153, '866927': 3154, '867850': 3155, '868529': 3156, '870544': 3157, '870545': 3158, '870546': 3159, '870938': 3160, '871118': 3161, '871421': 3162, '871511': 3163, '871512': 3164, '871513': 3165, '871514': 3166, '871600': 3167, '872306': 3168, '872307': 3169, '872308': 3170, '872309': 3171, '872310': 3172, '872317': 3173, '873687': 3174, '874593': 3175, '874598': 3176, '874990': 3177, '875604': 3178, '876223': 3179, '876224': 3180, '876225': 3181, '876226': 3182, '876663': 3183, '877513': 3184, '877514': 3185, '878715': 3186, '881701': 3187, '881910': 3188, '881911': 3189, '881912': 3190, '882624': 3191, '883336': 3192, '884878': 3193, '885543': 3194, '885553': 3195, '886067': 3196, '886396': 3197, '886398': 3198, '888063': 3199, '888630': 3200, '888682': 3201, '888689': 3202, '888761': 3203, '888763': 3204, '890213': 3205, '890214': 3206, '890215': 3207, '890371': 3208, '890372': 3209, '890374': 3210, '890375': 3211, '890825': 3212, '892076': 3213, '895291': 3214, '897348': 3215, '898656': 3216, '900196': 3217, '902198': 3218, '902200': 3219, '902202': 3220, '902839': 3221, '903283': 3222, '903284': 3223, '903285': 3224, '903286': 3225, '903955': 3226, '906979': 3227, '906980': 3228, '907671': 3229, '907684': 3230, '907686': 3231, '908741': 3232, '909361': 3233, '910928': 3234, '911429': 3235, '911871': 3236, '911989': 3237, '911990': 3238, '913102': 3239, '913363': 3240, '913364': 3241, '913521': 3242, '913526': 3243, '913964': 3244, '913966': 3245, '913967': 3246, '913968': 3247, '915310': 3248, '915351': 3249, '915810': 3250, '915821': 3251, '915822': 3252, '915980': 3253, '916885': 3254, '916886': 3255, '916887': 3256, '920607': 3257, '921042': 3258, '926958': 3259, '928477': 3260, '929627': 3261, '929893': 3262, '930247': 3263, '932281': 3264, '933780': 3265, '936341': 3266, '936994': 3267, '937953': 3268, '938125': 3269, '938126': 3270, '938129': 3271, '938436': 3272, '938566': 3273, '938567': 3274, '938570': 3275, '938574': 3276, '938575': 3277, '938576': 3278, '938657': 3279, '938659': 3280, '939129': 3281, '939130': 3282, '939131': 3283, '939207': 3284, '939210': 3285, '939661': 3286, '939662': 3287, '939663': 3288, '940585': 3289, '940590': 3290, '940591': 3291, '940592': 3292, '940616': 3293, '940638': 3294, '940664': 3295, '940665': 3296, '946277': 3297, '946671': 3298, '949053': 3299, '949243': 3300, '949296': 3301, '949297': 3302, '949298': 3303, '949299': 3304, '949510': 3305, '949511': 3306, '949629': 3307, '951675': 3308, '953404': 3309, '953413': 3310, '953420': 3311, '953427': 3312, '953428': 3313, '953562': 3314, '953609': 3315, '954553': 3316, '954796': 3317, '954805': 3318, '956011': 3319, '956012': 3320, '956013': 3321, '956014': 3322, '957096': 3323, '957098': 3324, '958016': 3325, '958514': 3326, '959437': 3327, '959500': 3328, '959502': 3329, '962600': 3330, '962601': 3331, '964752': 3332, '964753': 3333, '964848': 3334, '967021': 3335, '968432': 3336, '968935': 3337, '968936': 3338, '968937': 3339, '977007': 3340, '979195': 3341, '979196': 3342, '979197': 3343, '979199': 3344, '979553': 3345, '979554': 3346, '980041': 3347, '981655': 3348, '982726': 3349, '982727': 3350, '983310': 3351, '985823': 3352, '985824': 3353, '986193': 3354, '987308': 3355, '987768': 3356, '987769': 3357, '988408': 3358, '988459': 3359, '989092': 3360, '991114': 3361, '991329': 3362, '991331': 3363, '99197': 3364, '996122': 3365, '996537': 3366, '996597': 3367, '996600': 3368, '996606': 3369, '996613': 3370, '999545': 3371, '999546': 3372, '999547': 3373}, 'perishable': {'0': 0, '1': 1}, 'class': {'1002': 0, '1003': 1, '1004': 2, '1006': 3, '1008': 4, '1010': 5, '1012': 6, '1013': 7, '1014': 8, '1016': 9, '1018': 10, '1022': 11, '1024': 12, '1025': 13, '1026': 14, '1027': 15, '1028': 16, '1029': 17, '1030': 18, '1032': 19, '1033': 20, '1034': 21, '1035': 22, '1036': 23, '1038': 24, '1039': 25, '1040': 26, '1042': 27, '1044': 28, '1045': 29, '1046': 30, '1048': 31, '1050': 32, '1052': 33, '1054': 34, '1056': 35, '1058': 36, '1060': 37, '1062': 38, '1064': 39, '1066': 40, '1067': 41, '1068': 42, '1070': 43, '1072': 44, '1074': 45, '1075': 46, '1076': 47, '1077': 48, '1078': 49, '1080': 50, '1082': 51, '1083': 52, '1084': 53, '1086': 54, '1087': 55, '1088': 56, '1092': 57, '1093': 58, '1094': 59, '1096': 60, '1114': 61, '1115': 62, '1116': 63, '1118': 64, '1120': 65, '1122': 66, '1124': 67, '1126': 68, '1132': 69, '1136': 70, '1138': 71, '1142': 72, '1144': 73, '1146': 74, '1148': 75, '1152': 76, '1153': 77, '1154': 78, '1190': 79, '1236': 80, '1302': 81, '1310': 82, '1312': 83, '1314': 84, '1318': 85, '1320': 86, '1326': 87, '1330': 88, '1336': 89, '1338': 90, '1364': 91, '1380': 92, '1386': 93, '1387': 94, '2002': 95, '2004': 96, '2006': 97, '2008': 98, '2010': 99, '2012': 100, '2014': 101, '2016': 102, '2018': 103, '2020': 104, '2022': 105, '2024': 106, '2026': 107, '2028': 108, '2030': 109, '2032': 110, '2034': 111, '2070': 112, '2074': 113, '2102': 114, '2103': 115, '2104': 116, '2108': 117, '2112': 118, '2114': 119, '2116': 120, '2122': 121, '2124': 122, '2128': 123, '2130': 124, '2142': 125, '2152': 126, '2156': 127, '2162': 128, '2164': 129, '2166': 130, '2170': 131, '2172': 132, '2174': 133, '2178': 134, '2208': 135, '2210': 136, '2214': 137, '2218': 138, '2220': 139, '2222': 140, '2226': 141, '2228': 142, '2238': 143, '2242': 144, '2246': 145, '2302': 146, '2304': 147, '2306': 148, '2340': 149, '2372': 150, '2412': 151, '2416': 152, '2420': 153, '2502': 154, '2504': 155, '2506': 156, '2630': 157, '2632': 158, '2636': 159, '2640': 160, '2642': 161, '2644': 162, '2646': 163, '2650': 164, '2652': 165, '2654': 166, '2662': 167, '2664': 168, '2690': 169, '2702': 170, '2704': 171, '2708': 172, '2712': 173, '2714': 174, '2716': 175, '2718': 176, '2720': 177, '2722': 178, '2750': 179, '2752': 180, '2756': 181, '2782': 182, '2784': 183, '2786': 184, '2802': 185, '2806': 186, '2850': 187, '2854': 188, '2864': 189, '2904': 190, '2954': 191, '2956': 192, '2960': 193, '2962': 194, '2966': 195, '2970': 196, '2980': 197, '2986': 198, '3004': 199, '3005': 200, '3006': 201, '3008': 202, '3010': 203, '3012': 204, '3014': 205, '3015': 206, '3016': 207, '3018': 208, '3020': 209, '3022': 210, '3024': 211, '3026': 212, '3028': 213, '3029': 214, '3030': 215, '3032': 216, '3034': 217, '3035': 218, '3038': 219, '3040': 220, '3044': 221, '3046': 222, '3060': 223, '3090': 224, '3102': 225, '3104': 226, '3106': 227, '3108': 228, '3110': 229, '4104': 230, '4114': 231, '4118': 232, '4122': 233, '4126': 234, '4130': 235, '4138': 236, '4139': 237, '4140': 238, '4141': 239, '4162': 240, '4176': 241, '4210': 242, '4212': 243, '4214': 244, '4222': 245, '4228': 246, '4250': 247, '4252': 248, '4254': 249, '5222': 250, '5224': 251, '5244': 252, '5250': 253, '5308': 254, '5322': 255, '5324': 256, '5446': 257, '6022': 258, '6155': 259, '6206': 260, '6212': 261, '6222': 262, '6223': 263, '6230': 264, '6232': 265, '6238': 266, '6241': 267, '6242': 268, '6246': 269, '6248': 270, '6253': 271, '6257': 272, '6258': 273, '6260': 274, '6262': 275, '6266': 276, '6267': 277, '6269': 278, '6301': 279, '6322': 280, '6328': 281, '6329': 282, '6330': 283, '6338': 284, '6344': 285, '6350': 286, '6352': 287, '6353': 288, '6392': 289, '6393': 290, '6404': 291, '6412': 292, '6414': 293, '6426': 294, '6448': 295, '6482': 296, '6516': 297, '6517': 298, '6706': 299, '6806': 300, '6808': 301, '6810': 302, '6824': 303, '6848': 304, '6918': 305, '6920': 306, '6954': 307, '7002': 308, '7016': 309, '7034': 310, '7780': 311}, 'cluster': {'11': 0, '12': 1, '13': 2, '14': 3, '15': 4, '16': 5, '5': 6, '6': 7, '8': 8, '9': 9}, 'weekday': {'0': 0, '1': 1}, 'holiday_description': {'Batalla de Pichincha': 0, 'Cantonizacion de Cayambe': 1, 'Cantonizacion de El Carmen': 2, 'Cantonizacion de Guaranda': 3, 'Cantonizacion de Latacunga': 4, 'Cantonizacion de Libertad': 5, 'Cantonizacion de Quevedo': 6, 'Cantonizacion de Riobamba': 7, 'Cantonizacion del Puyo': 8, 'Carnaval': 9, 'Dia de la Madre': 10, 'Dia de la Madre-1': 11, 'Dia del Trabajo': 12, 'Fundacion de Ambato': 13, 'Fundacion de Cuenca': 14, 'Fundacion de Esmeraldas': 15, 'Fundacion de Guayaquil': 16, 'Fundacion de Guayaquil-1': 17, 'Fundacion de Ibarra': 18, 'Fundacion de Machala': 19, 'Fundacion de Manta': 20, 'Fundacion de Riobamba': 21, 'Fundacion de Santo Domingo': 22, 'Independencia de Guayaquil': 23, 'Primer Grito de Independencia': 24, 'Provincializacion de Cotopaxi': 25, 'Provincializacion de Imbabura': 26, 'Puente Primer dia del ano': 27, 'Recupero Puente Primer dia del ano': 28, 'Viernes Santo': 29, 'nan': 30}, 'holiday_type': {'Additional': 0, 'Bridge': 1, 'Event': 2, 'Holiday': 3, 'Work Day': 4, 'nan': 5}, 'onpromotion': {'0': 0, '1': 1}}\n",
            "  \t\"embedding_paddings\":                []\n",
            "  \t\"embedding_sizes\":                   {'store_nbr': (18, 8), 'family': (32, 11), 'store_type': (4, 3), 'item_nbr': (3374, 100), 'perishable': (2, 1), 'class': (312, 40), 'cluster': (10, 6), 'weekday': (2, 1), 'holiday_description': (31, 11), 'holiday_type': (6, 4), 'onpromotion': (2, 1)}\n",
            "  \t\"hidden_continuous_size\":            8\n",
            "  \t\"hidden_continuous_sizes\":           {}\n",
            "  \t\"hidden_size\":                       16\n",
            "  \t\"learning_rate\":                     0.001\n",
            "  \t\"log_gradient_flow\":                 False\n",
            "  \t\"log_interval\":                      -1\n",
            "  \t\"log_val_interval\":                  -1\n",
            "  \t\"lstm_layers\":                       1\n",
            "  \t\"max_encoder_length\":                30\n",
            "  \t\"monotone_constaints\":               {}\n",
            "  \t\"optimizer\":                         adam\n",
            "  \t\"optimizer_params\":                  None\n",
            "  \t\"output_size\":                       7\n",
            "  \t\"output_transformer\":                EncoderNormalizer(\n",
            "  \t\tmethod='standard',\n",
            "  \t\tcenter=True,\n",
            "  \t\tmax_length=None,\n",
            "  \t\ttransformation=None,\n",
            "  \t\tmethod_kwargs={}\n",
            "  \t)\n",
            "  \t\"reduce_on_plateau_min_lr\":          1e-05\n",
            "  \t\"reduce_on_plateau_patience\":        1000\n",
            "  \t\"reduce_on_plateau_reduction\":       2.0\n",
            "  \t\"share_single_variable_networks\":    False\n",
            "  \t\"static_categoricals\":               ['store_nbr', 'family', 'store_type', 'item_nbr', 'perishable', 'class', 'cluster']\n",
            "  \t\"static_reals\":                      ['encoder_length']\n",
            "  \t\"time_varying_categoricals_decoder\": ['weekday', 'holiday_description', 'holiday_type', 'onpromotion']\n",
            "  \t\"time_varying_categoricals_encoder\": ['weekday', 'holiday_description', 'holiday_type', 'onpromotion']\n",
            "  \t\"time_varying_reals_decoder\":        ['dcoilwtico', 'Min Temperature (C)', 'Max Temperature (C)', 'relative_time_idx']\n",
            "  \t\"time_varying_reals_encoder\":        ['dcoilwtico', 'Min Temperature (C)', 'Max Temperature (C)', 'relative_time_idx']\n",
            "  \t\"weight_decay\":                      0.0\n",
            "  \t\"x_categoricals\":                    ['store_nbr', 'family', 'store_type', 'item_nbr', 'perishable', 'class', 'cluster', 'weekday', 'holiday_description', 'holiday_type', 'onpromotion']\n",
            "  \t\"x_reals\":                           ['encoder_length', 'dcoilwtico', 'Min Temperature (C)', 'Max Temperature (C)', 'relative_time_idx']\n",
            "  (loss): QuantileLoss(quantiles=[0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98])\n",
            "  (logging_metrics): ModuleList(\n",
            "    (0): SMAPE()\n",
            "    (1): MAE()\n",
            "    (2): RMSE()\n",
            "    (3): MAPE()\n",
            "  )\n",
            "  (input_embeddings): MultiEmbedding(\n",
            "    (embeddings): ModuleDict(\n",
            "      (store_nbr): Embedding(18, 8)\n",
            "      (family): Embedding(32, 11)\n",
            "      (store_type): Embedding(4, 3)\n",
            "      (item_nbr): Embedding(3374, 16)\n",
            "      (perishable): Embedding(2, 1)\n",
            "      (class): Embedding(312, 16)\n",
            "      (cluster): Embedding(10, 6)\n",
            "      (weekday): Embedding(2, 1)\n",
            "      (holiday_description): Embedding(31, 11)\n",
            "      (holiday_type): Embedding(6, 4)\n",
            "      (onpromotion): Embedding(2, 1)\n",
            "    )\n",
            "  )\n",
            "  (prescalers): ModuleDict(\n",
            "    (encoder_length): Linear(in_features=1, out_features=8, bias=True)\n",
            "    (dcoilwtico): Linear(in_features=1, out_features=8, bias=True)\n",
            "    (Min Temperature (C)): Linear(in_features=1, out_features=8, bias=True)\n",
            "    (Max Temperature (C)): Linear(in_features=1, out_features=8, bias=True)\n",
            "    (relative_time_idx): Linear(in_features=1, out_features=8, bias=True)\n",
            "  )\n",
            "  (static_variable_selection): VariableSelectionNetwork(\n",
            "    (flattened_grn): GatedResidualNetwork(\n",
            "      (resample_norm): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (fc1): Linear(in_features=69, out_features=8, bias=True)\n",
            "      (elu): ELU(alpha=1.0)\n",
            "      (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
            "      (gate_norm): GateAddNorm(\n",
            "        (glu): GatedLinearUnit(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (fc): Linear(in_features=8, out_features=16, bias=True)\n",
            "        )\n",
            "        (add_norm): AddNorm(\n",
            "          (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (single_variable_grns): ModuleDict(\n",
            "      (store_nbr): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (family): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (store_type): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (item_nbr): ResampleNorm(\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (perishable): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (class): ResampleNorm(\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (cluster): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (encoder_length): GatedResidualNetwork(\n",
            "        (resample_norm): ResampleNorm(\n",
            "          (resample): TimeDistributedInterpolation()\n",
            "          (gate): Sigmoid()\n",
            "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (elu): ELU(alpha=1.0)\n",
            "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (gate_norm): GateAddNorm(\n",
            "          (glu): GatedLinearUnit(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
            "          )\n",
            "          (add_norm): AddNorm(\n",
            "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (prescalers): ModuleDict(\n",
            "      (encoder_length): Linear(in_features=1, out_features=8, bias=True)\n",
            "    )\n",
            "    (softmax): Softmax(dim=-1)\n",
            "  )\n",
            "  (encoder_variable_selection): VariableSelectionNetwork(\n",
            "    (flattened_grn): GatedResidualNetwork(\n",
            "      (resample_norm): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (fc1): Linear(in_features=49, out_features=8, bias=True)\n",
            "      (elu): ELU(alpha=1.0)\n",
            "      (context): Linear(in_features=16, out_features=8, bias=False)\n",
            "      (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
            "      (gate_norm): GateAddNorm(\n",
            "        (glu): GatedLinearUnit(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (fc): Linear(in_features=8, out_features=16, bias=True)\n",
            "        )\n",
            "        (add_norm): AddNorm(\n",
            "          (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (single_variable_grns): ModuleDict(\n",
            "      (weekday): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (holiday_description): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (holiday_type): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (onpromotion): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (dcoilwtico): GatedResidualNetwork(\n",
            "        (resample_norm): ResampleNorm(\n",
            "          (resample): TimeDistributedInterpolation()\n",
            "          (gate): Sigmoid()\n",
            "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (elu): ELU(alpha=1.0)\n",
            "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (gate_norm): GateAddNorm(\n",
            "          (glu): GatedLinearUnit(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
            "          )\n",
            "          (add_norm): AddNorm(\n",
            "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (Min Temperature (C)): GatedResidualNetwork(\n",
            "        (resample_norm): ResampleNorm(\n",
            "          (resample): TimeDistributedInterpolation()\n",
            "          (gate): Sigmoid()\n",
            "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (elu): ELU(alpha=1.0)\n",
            "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (gate_norm): GateAddNorm(\n",
            "          (glu): GatedLinearUnit(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
            "          )\n",
            "          (add_norm): AddNorm(\n",
            "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (Max Temperature (C)): GatedResidualNetwork(\n",
            "        (resample_norm): ResampleNorm(\n",
            "          (resample): TimeDistributedInterpolation()\n",
            "          (gate): Sigmoid()\n",
            "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (elu): ELU(alpha=1.0)\n",
            "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (gate_norm): GateAddNorm(\n",
            "          (glu): GatedLinearUnit(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
            "          )\n",
            "          (add_norm): AddNorm(\n",
            "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (relative_time_idx): GatedResidualNetwork(\n",
            "        (resample_norm): ResampleNorm(\n",
            "          (resample): TimeDistributedInterpolation()\n",
            "          (gate): Sigmoid()\n",
            "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (elu): ELU(alpha=1.0)\n",
            "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (gate_norm): GateAddNorm(\n",
            "          (glu): GatedLinearUnit(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
            "          )\n",
            "          (add_norm): AddNorm(\n",
            "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (prescalers): ModuleDict(\n",
            "      (dcoilwtico): Linear(in_features=1, out_features=8, bias=True)\n",
            "      (Min Temperature (C)): Linear(in_features=1, out_features=8, bias=True)\n",
            "      (Max Temperature (C)): Linear(in_features=1, out_features=8, bias=True)\n",
            "      (relative_time_idx): Linear(in_features=1, out_features=8, bias=True)\n",
            "    )\n",
            "    (softmax): Softmax(dim=-1)\n",
            "  )\n",
            "  (decoder_variable_selection): VariableSelectionNetwork(\n",
            "    (flattened_grn): GatedResidualNetwork(\n",
            "      (resample_norm): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (fc1): Linear(in_features=49, out_features=8, bias=True)\n",
            "      (elu): ELU(alpha=1.0)\n",
            "      (context): Linear(in_features=16, out_features=8, bias=False)\n",
            "      (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
            "      (gate_norm): GateAddNorm(\n",
            "        (glu): GatedLinearUnit(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (fc): Linear(in_features=8, out_features=16, bias=True)\n",
            "        )\n",
            "        (add_norm): AddNorm(\n",
            "          (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (single_variable_grns): ModuleDict(\n",
            "      (weekday): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (holiday_description): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (holiday_type): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (onpromotion): ResampleNorm(\n",
            "        (resample): TimeDistributedInterpolation()\n",
            "        (gate): Sigmoid()\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (dcoilwtico): GatedResidualNetwork(\n",
            "        (resample_norm): ResampleNorm(\n",
            "          (resample): TimeDistributedInterpolation()\n",
            "          (gate): Sigmoid()\n",
            "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (elu): ELU(alpha=1.0)\n",
            "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (gate_norm): GateAddNorm(\n",
            "          (glu): GatedLinearUnit(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
            "          )\n",
            "          (add_norm): AddNorm(\n",
            "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (Min Temperature (C)): GatedResidualNetwork(\n",
            "        (resample_norm): ResampleNorm(\n",
            "          (resample): TimeDistributedInterpolation()\n",
            "          (gate): Sigmoid()\n",
            "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (elu): ELU(alpha=1.0)\n",
            "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (gate_norm): GateAddNorm(\n",
            "          (glu): GatedLinearUnit(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
            "          )\n",
            "          (add_norm): AddNorm(\n",
            "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (Max Temperature (C)): GatedResidualNetwork(\n",
            "        (resample_norm): ResampleNorm(\n",
            "          (resample): TimeDistributedInterpolation()\n",
            "          (gate): Sigmoid()\n",
            "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (elu): ELU(alpha=1.0)\n",
            "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (gate_norm): GateAddNorm(\n",
            "          (glu): GatedLinearUnit(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
            "          )\n",
            "          (add_norm): AddNorm(\n",
            "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (relative_time_idx): GatedResidualNetwork(\n",
            "        (resample_norm): ResampleNorm(\n",
            "          (resample): TimeDistributedInterpolation()\n",
            "          (gate): Sigmoid()\n",
            "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (elu): ELU(alpha=1.0)\n",
            "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
            "        (gate_norm): GateAddNorm(\n",
            "          (glu): GatedLinearUnit(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
            "          )\n",
            "          (add_norm): AddNorm(\n",
            "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (prescalers): ModuleDict(\n",
            "      (dcoilwtico): Linear(in_features=1, out_features=8, bias=True)\n",
            "      (Min Temperature (C)): Linear(in_features=1, out_features=8, bias=True)\n",
            "      (Max Temperature (C)): Linear(in_features=1, out_features=8, bias=True)\n",
            "      (relative_time_idx): Linear(in_features=1, out_features=8, bias=True)\n",
            "    )\n",
            "    (softmax): Softmax(dim=-1)\n",
            "  )\n",
            "  (static_context_variable_selection): GatedResidualNetwork(\n",
            "    (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (elu): ELU(alpha=1.0)\n",
            "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (gate_norm): GateAddNorm(\n",
            "      (glu): GatedLinearUnit(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (fc): Linear(in_features=16, out_features=32, bias=True)\n",
            "      )\n",
            "      (add_norm): AddNorm(\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (static_context_initial_hidden_lstm): GatedResidualNetwork(\n",
            "    (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (elu): ELU(alpha=1.0)\n",
            "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (gate_norm): GateAddNorm(\n",
            "      (glu): GatedLinearUnit(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (fc): Linear(in_features=16, out_features=32, bias=True)\n",
            "      )\n",
            "      (add_norm): AddNorm(\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (static_context_initial_cell_lstm): GatedResidualNetwork(\n",
            "    (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (elu): ELU(alpha=1.0)\n",
            "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (gate_norm): GateAddNorm(\n",
            "      (glu): GatedLinearUnit(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (fc): Linear(in_features=16, out_features=32, bias=True)\n",
            "      )\n",
            "      (add_norm): AddNorm(\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (static_context_enrichment): GatedResidualNetwork(\n",
            "    (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (elu): ELU(alpha=1.0)\n",
            "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (gate_norm): GateAddNorm(\n",
            "      (glu): GatedLinearUnit(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (fc): Linear(in_features=16, out_features=32, bias=True)\n",
            "      )\n",
            "      (add_norm): AddNorm(\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lstm_encoder): LSTM(16, 16, batch_first=True)\n",
            "  (lstm_decoder): LSTM(16, 16, batch_first=True)\n",
            "  (post_lstm_gate_encoder): GatedLinearUnit(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (fc): Linear(in_features=16, out_features=32, bias=True)\n",
            "  )\n",
            "  (post_lstm_gate_decoder): GatedLinearUnit(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (fc): Linear(in_features=16, out_features=32, bias=True)\n",
            "  )\n",
            "  (post_lstm_add_norm_encoder): AddNorm(\n",
            "    (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (post_lstm_add_norm_decoder): AddNorm(\n",
            "    (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (static_enrichment): GatedResidualNetwork(\n",
            "    (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (elu): ELU(alpha=1.0)\n",
            "    (context): Linear(in_features=16, out_features=16, bias=False)\n",
            "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (gate_norm): GateAddNorm(\n",
            "      (glu): GatedLinearUnit(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (fc): Linear(in_features=16, out_features=32, bias=True)\n",
            "      )\n",
            "      (add_norm): AddNorm(\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (multihead_attn): InterpretableMultiHeadAttention(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (v_layer): Linear(in_features=16, out_features=4, bias=True)\n",
            "    (q_layers): ModuleList(\n",
            "      (0-3): 4 x Linear(in_features=16, out_features=4, bias=True)\n",
            "    )\n",
            "    (k_layers): ModuleList(\n",
            "      (0-3): 4 x Linear(in_features=16, out_features=4, bias=True)\n",
            "    )\n",
            "    (attention): ScaledDotProductAttention(\n",
            "      (softmax): Softmax(dim=2)\n",
            "    )\n",
            "    (w_h): Linear(in_features=4, out_features=16, bias=False)\n",
            "  )\n",
            "  (post_attn_gate_norm): GateAddNorm(\n",
            "    (glu): GatedLinearUnit(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (fc): Linear(in_features=16, out_features=32, bias=True)\n",
            "    )\n",
            "    (add_norm): AddNorm(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (pos_wise_ff): GatedResidualNetwork(\n",
            "    (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (elu): ELU(alpha=1.0)\n",
            "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (gate_norm): GateAddNorm(\n",
            "      (glu): GatedLinearUnit(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (fc): Linear(in_features=16, out_features=32, bias=True)\n",
            "      )\n",
            "      (add_norm): AddNorm(\n",
            "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pre_output_gate_norm): GateAddNorm(\n",
            "    (glu): GatedLinearUnit(\n",
            "      (fc): Linear(in_features=16, out_features=32, bias=True)\n",
            "    )\n",
            "    (add_norm): AddNorm(\n",
            "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (output_layer): Linear(in_features=16, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6QvHHt2XAyp"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "568f5a568563472cb9dd932660afb495",
            "0318347693bb46b590941ede8e9dc152",
            "64a1696ff33f4066be1cc274896e5c97",
            "12288b6ea5584c7bbe37eae92a4f0a41",
            "06b869edd643431fae3b6e463cb79895",
            "a17256f0b48447adb9db0efea3d49be8",
            "96053004d8934f899384b67c13cc0d3b",
            "d6e6f03fe5614f7e95f1297ec18a1dcd",
            "f8e3bdb28a1b461bb5c6d7b05cfdf1a1",
            "ebd991925d224addb594c87e89628c80",
            "c798b2add22c4509aaa7168487221eae",
            "8ade8459cb104549bad487254ecc9ef1",
            "4da40c98565d4cd9aec1949f956d8ff2",
            "69bbd01d8ee241919176535777ab4ccc",
            "094ec6aa9e6e4737bd79aef6000b6467",
            "9dde5dc2db474323bd9148bb68f71a11",
            "b9f5300f63b7480c98f8ff19ee98f7c2",
            "10d31313456146f0a347f8d4674770b4",
            "c4884154f7ed486fae30e5158146841d",
            "136bacfa645748c8a6832ae96c40d03c",
            "458606c30b484fa5850d299dd7b2af9e",
            "11b7f2129d844d579ef5964e4ba27c23"
          ]
        },
        "id": "9TbbgtXwgRpi",
        "outputId": "46bf414b-0f19-4fed-dab0-d64ad1907165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "INFO: \n",
            "   | Name                               | Type                            | Params | Mode \n",
            "------------------------------------------------------------------------------------------------\n",
            "0  | loss                               | QuantileLoss                    | 0      | train\n",
            "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
            "2  | input_embeddings                   | MultiEmbedding                  | 59.9 K | train\n",
            "3  | prescalers                         | ModuleDict                      | 80     | train\n",
            "4  | static_variable_selection          | VariableSelectionNetwork        | 1.7 K  | train\n",
            "5  | encoder_variable_selection         | VariableSelectionNetwork        | 3.1 K  | train\n",
            "6  | decoder_variable_selection         | VariableSelectionNetwork        | 3.1 K  | train\n",
            "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
            "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
            "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
            "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
            "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
            "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
            "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
            "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
            "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
            "16 | multihead_attn                     | InterpretableMultiHeadAttention | 676    | train\n",
            "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
            "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
            "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
            "20 | output_layer                       | Linear                          | 119    | train\n",
            "------------------------------------------------------------------------------------------------\n",
            "81.6 K    Trainable params\n",
            "0         Non-trainable params\n",
            "81.6 K    Total params\n",
            "0.326     Total estimated model params size (MB)\n",
            "361       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "   | Name                               | Type                            | Params | Mode \n",
            "------------------------------------------------------------------------------------------------\n",
            "0  | loss                               | QuantileLoss                    | 0      | train\n",
            "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
            "2  | input_embeddings                   | MultiEmbedding                  | 59.9 K | train\n",
            "3  | prescalers                         | ModuleDict                      | 80     | train\n",
            "4  | static_variable_selection          | VariableSelectionNetwork        | 1.7 K  | train\n",
            "5  | encoder_variable_selection         | VariableSelectionNetwork        | 3.1 K  | train\n",
            "6  | decoder_variable_selection         | VariableSelectionNetwork        | 3.1 K  | train\n",
            "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
            "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
            "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
            "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
            "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
            "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
            "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
            "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
            "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
            "16 | multihead_attn                     | InterpretableMultiHeadAttention | 676    | train\n",
            "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
            "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
            "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
            "20 | output_layer                       | Linear                          | 119    | train\n",
            "------------------------------------------------------------------------------------------------\n",
            "81.6 K    Trainable params\n",
            "0         Non-trainable params\n",
            "81.6 K    Total params\n",
            "0.326     Total estimated model params size (MB)\n",
            "361       Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "568f5a568563472cb9dd932660afb495"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ade8459cb104549bad487254ecc9ef1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'exit' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m         )\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0;31m# in automatic optimization, there can only be one optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m_optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# model hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         call._call_lightning_module_hook(\n\u001b[0m\u001b[1;32m    269\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \"\"\"\n\u001b[0;32m-> 1306\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mclosure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_closure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py\u001b[0m in \u001b[0;36m_wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \"\"\"\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mclosure_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mClosureResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m_training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# unused hook - call anyway for backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_forecasting/models/base_model.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_forecasting/models/base_model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, x, y, batch_idx, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    462\u001b[0m         }\n\u001b[0;32m--> 463\u001b[0;31m         embeddings_varying_encoder, encoder_sparse_weights = self.encoder_variable_selection(\n\u001b[0m\u001b[1;32m    464\u001b[0m             \u001b[0membeddings_varying_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0mweight_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0mvar_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_variable_grns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mvar_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2899\u001b[0m         )\n\u001b[0;32m-> 2900\u001b[0;31m     return torch.layer_norm(\n\u001b[0m\u001b[1;32m   2901\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-83555909c0b2>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SubprocessScriptLauncher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_sigkill_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
          ]
        }
      ],
      "source": [
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define a ModelCheckpoint callback to save the best model based on validation loss\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor=\"val_loss\",  # Metric to monitor\n",
        "    dirpath=\"checkpoints/\",  # Directory to save the model checkpoints\n",
        "    filename=\"best_model-{epoch:02d}-{val_loss:.2f}\",  # Filename pattern for saved models\n",
        "    save_top_k=1,  # Save only the best model\n",
        "    mode=\"min\",  # Minimizing validation loss\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=3,\n",
        "    gradient_clip_val=0.1,\n",
        "    limit_train_batches=1.0,  # Use the full training dataset\n",
        "    callbacks=[checkpoint_callback],  # Add the checkpoint callback\n",
        ")\n",
        "\n",
        "trainer.fit(tft, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "checkpoint_path = path + \"tft_model_mini.ckpt\"\n",
        "trainer.save_checkpoint(checkpoint_path)"
      ],
      "metadata": {
        "id": "R-0RA2kD-ctK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "568f5a568563472cb9dd932660afb495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0318347693bb46b590941ede8e9dc152",
              "IPY_MODEL_64a1696ff33f4066be1cc274896e5c97",
              "IPY_MODEL_12288b6ea5584c7bbe37eae92a4f0a41"
            ],
            "layout": "IPY_MODEL_06b869edd643431fae3b6e463cb79895"
          }
        },
        "0318347693bb46b590941ede8e9dc152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a17256f0b48447adb9db0efea3d49be8",
            "placeholder": "​",
            "style": "IPY_MODEL_96053004d8934f899384b67c13cc0d3b",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "64a1696ff33f4066be1cc274896e5c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6e6f03fe5614f7e95f1297ec18a1dcd",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8e3bdb28a1b461bb5c6d7b05cfdf1a1",
            "value": 2
          }
        },
        "12288b6ea5584c7bbe37eae92a4f0a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebd991925d224addb594c87e89628c80",
            "placeholder": "​",
            "style": "IPY_MODEL_c798b2add22c4509aaa7168487221eae",
            "value": " 2/2 [00:00&lt;00:00,  4.66it/s]"
          }
        },
        "06b869edd643431fae3b6e463cb79895": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "a17256f0b48447adb9db0efea3d49be8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96053004d8934f899384b67c13cc0d3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6e6f03fe5614f7e95f1297ec18a1dcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8e3bdb28a1b461bb5c6d7b05cfdf1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ebd991925d224addb594c87e89628c80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c798b2add22c4509aaa7168487221eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ade8459cb104549bad487254ecc9ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4da40c98565d4cd9aec1949f956d8ff2",
              "IPY_MODEL_69bbd01d8ee241919176535777ab4ccc",
              "IPY_MODEL_094ec6aa9e6e4737bd79aef6000b6467"
            ],
            "layout": "IPY_MODEL_9dde5dc2db474323bd9148bb68f71a11"
          }
        },
        "4da40c98565d4cd9aec1949f956d8ff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9f5300f63b7480c98f8ff19ee98f7c2",
            "placeholder": "​",
            "style": "IPY_MODEL_10d31313456146f0a347f8d4674770b4",
            "value": "Epoch 0:   0%"
          }
        },
        "69bbd01d8ee241919176535777ab4ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4884154f7ed486fae30e5158146841d",
            "max": 255984,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_136bacfa645748c8a6832ae96c40d03c",
            "value": 2
          }
        },
        "094ec6aa9e6e4737bd79aef6000b6467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_458606c30b484fa5850d299dd7b2af9e",
            "placeholder": "​",
            "style": "IPY_MODEL_11b7f2129d844d579ef5964e4ba27c23",
            "value": " 2/255984 [00:36&lt;1285:45:03,  0.06it/s, v_num=0, train_loss_step=0.000595]"
          }
        },
        "9dde5dc2db474323bd9148bb68f71a11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "b9f5300f63b7480c98f8ff19ee98f7c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10d31313456146f0a347f8d4674770b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4884154f7ed486fae30e5158146841d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "136bacfa645748c8a6832ae96c40d03c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "458606c30b484fa5850d299dd7b2af9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b7f2129d844d579ef5964e4ba27c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}